{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e3abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[660_RPM] Loaded tokens: 115752 | Feature dim: 21\n",
      "[660_RPM] Class counts (tokens): {'BF': 23932, 'GF': 40187, 'TF': 25855, 'N': 25778}\n",
      "[660_RPM] Train tokens: 73505 | Val tokens: 13154 | Test tokens: 29093\n",
      "[660_RPM] Train sample files: 369 | Val sample files: 66 | Test sample files: 146\n",
      "[660_RPM] Epoch 01 | train loss 1.2724 acc 67.09% | val loss 1.2363 acc 74.35%\n",
      "[660_RPM] Epoch 05 | train loss 0.9182 acc 75.56% | val loss 0.8924 acc 78.38%\n",
      "[660_RPM] Epoch 10 | train loss 0.8725 acc 77.05% | val loss 0.8547 acc 79.19%\n",
      "[660_RPM] Epoch 15 | train loss 0.8592 acc 82.89% | val loss 0.8467 acc 83.59%\n",
      "[660_RPM] Epoch 20 | train loss 0.8556 acc 83.19% | val loss 0.8436 acc 84.23%\n",
      "[660_RPM] Epoch 25 | train loss 0.8477 acc 77.93% | val loss 0.8389 acc 78.92%\n",
      "[660_RPM] Epoch 30 | train loss 0.8484 acc 71.53% | val loss 0.8410 acc 74.71%\n",
      "    ✓ Saved: 660_RPM_training_curves.png\n",
      "    ✓ Saved: 660_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_EMB_3D_UMAP.png and 660_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[660_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        30\n",
      "          GF     1.0000    0.8200    0.9011        50\n",
      "          TF     1.0000    1.0000    1.0000        33\n",
      "           N     0.7857    1.0000    0.8800        33\n",
      "\n",
      "    accuracy                         0.9384       146\n",
      "   macro avg     0.9464    0.9550    0.9453       146\n",
      "weighted avg     0.9516    0.9384    0.9390       146\n",
      "\n",
      "[660_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
      "\n",
      "[720_RPM] Loaded tokens: 133959 | Feature dim: 21\n",
      "[720_RPM] Class counts (tokens): {'BF': 30458, 'GF': 32960, 'TF': 36839, 'N': 33702}\n",
      "[720_RPM] Train tokens: 85332 | Val tokens: 15145 | Test tokens: 33482\n",
      "[720_RPM] Train sample files: 428 | Val sample files: 76 | Test sample files: 168\n",
      "[720_RPM] Epoch 01 | train loss 1.1886 acc 75.69% | val loss 1.1542 acc 81.25%\n",
      "[720_RPM] Epoch 05 | train loss 0.7389 acc 76.59% | val loss 0.7370 acc 76.25%\n",
      "[720_RPM] Epoch 10 | train loss 0.7277 acc 76.83% | val loss 0.7293 acc 75.53%\n",
      "[720_RPM] Epoch 15 | train loss 0.7269 acc 77.52% | val loss 0.7290 acc 78.80%\n",
      "[720_RPM] Epoch 20 | train loss 0.7232 acc 77.85% | val loss 0.7257 acc 76.26%\n",
      "[720_RPM] Epoch 25 | train loss 0.7215 acc 77.61% | val loss 0.7253 acc 76.56%\n",
      "[720_RPM] Epoch 30 | train loss 0.7210 acc 77.10% | val loss 0.7240 acc 76.59%\n",
      "    ✓ Saved: 720_RPM_training_curves.png\n",
      "    ✓ Saved: 720_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_EMB_3D_UMAP.png and 720_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[720_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        38\n",
      "          GF     0.6780    0.9756    0.8000        41\n",
      "          TF     1.0000    1.0000    1.0000        47\n",
      "           N     0.9583    0.5476    0.6970        42\n",
      "\n",
      "    accuracy                         0.8810       168\n",
      "   macro avg     0.9091    0.8808    0.8742       168\n",
      "weighted avg     0.9110    0.8810    0.8754       168\n",
      "\n",
      "[720_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n",
      "\n",
      "✅ DONE for BOTH 660 & 720 RPM.\n",
      "660 outputs: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
      "720 outputs: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EPT-AE (Energy–Peak Tokenization + Regularized Prototype Classifier)\n",
    "# SAMPLE-LEVEL pipeline for BOTH 660 & 720 RPM\n",
    "# End-to-end in ONE CELL:\n",
    "# - Load .mat AE signals\n",
    "# - Energy-peak tokenization -> event tokens\n",
    "# - Compact features (time + spectral + WPT top-k)\n",
    "# - Train embedding + prototype regularization\n",
    "# - SAMPLE-level inference (distance-weighted voting)\n",
    "# - Save results + plots (CM, ROC, t-SNE 2D/3D, training curves)\n",
    "#\n",
    "# Outputs:\n",
    "#   660 -> E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
    "#   720 -> E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, json, math\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import pywt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# -------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tokenization\n",
    "ENERGY_WIN = 256\n",
    "PEAK_DISTANCE = 800\n",
    "K_MAD = 6.0\n",
    "\n",
    "SEG_LEN = 4096\n",
    "MAX_TOKENS_PER_FILE = 200\n",
    "\n",
    "# Feature extraction\n",
    "WPT_WAVELET = \"db4\"\n",
    "WPT_LEVEL = 5\n",
    "TOPK_WPT = 10\n",
    "FFT_N = 2048\n",
    "\n",
    "# Training\n",
    "TEST_SIZE = 0.25\n",
    "VAL_SIZE_FROM_TRAIN = 0.15\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "LAMBDA_COMPACT = 0.2\n",
    "\n",
    "# Sample-level voting\n",
    "ALPHA_VOTE = 15.0  # softmax temperature for weighted voting\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# REPRODUCIBILITY\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VISUALIZATION CLASS (your style + saving)\n",
    "# -------------------------\n",
    "CLASS_NAMES = [\"BF\", \"GF\", \"TF\", \"N\"]  # desired order\n",
    "# If your internal label order differs, adjust mapping here:\n",
    "# ORIGINAL_TO_NEW: old_label_index -> new_label_index (BF=0, GF=1, TF=2, N=3)\n",
    "# We'll build it dynamically per experiment, but default identity:\n",
    "ORIGINAL_TO_NEW = {0:0, 1:1, 2:2, 3:3}\n",
    "\n",
    "class PublicationVisualizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def remap_labels(labels):\n",
    "        return np.array([ORIGINAL_TO_NEW[int(label)] for label in labels])\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, output_dir, filename):\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "        y_pred_remapped = PublicationVisualizer.remap_labels(y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_true_remapped, y_pred_remapped)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                    cbar=False, annot_kws={\"size\": 22, \"fontweight\": \"bold\"})\n",
    "        plt.xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "        plt.setp(plt.gca().get_xticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.setp(plt.gca().get_yticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(y_true, y_proba, output_dir, filename):\n",
    "        # y_true: original indices, y_proba: shape [N, C] in original order\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        y_proba_remapped = np.zeros_like(y_proba)\n",
    "        for old_idx, new_idx in ORIGINAL_TO_NEW.items():\n",
    "            y_proba_remapped[:, new_idx] = y_proba[:, old_idx]\n",
    "\n",
    "        y_bin = label_binarize(y_true_remapped, classes=[0, 1, 2, 3])\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "        for i in range(4):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba_remapped[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2.5, color=colors[i], linestyle=line_styles[i],\n",
    "                     label=f'{CLASS_NAMES[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=13, frameon=True, framealpha=0.95)\n",
    "        plt.grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.xlim([-0.02, 1.02])\n",
    "        plt.ylim([-0.02, 1.02])\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_2d(features, y_true, output_dir, filename):\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        markers = ['o', 's', '^', 'D']\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "        tsne = TSNE(\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            init='pca',\n",
    "            learning_rate=200,\n",
    "            perplexity=perplexity,\n",
    "            n_iter=3000,\n",
    "            early_exaggeration=12.0,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        features_2d = tsne.fit_transform(features)\n",
    "\n",
    "        plt.figure(figsize=(8, 7))\n",
    "        for i, (cname, m, col) in enumerate(zip(CLASS_NAMES, markers, colors)):\n",
    "            sel = (y_true_remapped == i)\n",
    "            plt.scatter(features_2d[sel, 0], features_2d[sel, 1],\n",
    "                        marker=m, color=col, label=cname, alpha=0.85, s=80,\n",
    "                        edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        plt.legend(title=\"Fault Types\", loc='best',\n",
    "                   prop={'weight': 'bold', 'size': 14}, title_fontsize=15,\n",
    "                   frameon=True, fancybox=True, shadow=True)\n",
    "        plt.xlabel('t-SNE Component 1', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('t-SNE Component 2', fontsize=18, fontweight='bold')\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.grid(alpha=0.2, linestyle='--', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_3d(features, y_true, output_dir, filename_prefix):\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        rcParams['font.family'] = 'Arial'\n",
    "        rcParams['font.size'] = 12\n",
    "        colors_3d = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        # Prefer UMAP if installed; else 3D t-SNE\n",
    "        coords_3d = None\n",
    "        method = None\n",
    "        try:\n",
    "            import umap.umap_ as umap\n",
    "            reducer = umap.UMAP(\n",
    "                n_components=3,\n",
    "                n_neighbors=15,\n",
    "                min_dist=0.3,\n",
    "                metric=\"euclidean\",\n",
    "                random_state=42,\n",
    "                spread=1.5\n",
    "            )\n",
    "            coords_3d = reducer.fit_transform(features)\n",
    "            method = \"UMAP\"\n",
    "        except Exception:\n",
    "            perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "            tsne3 = TSNE(\n",
    "                n_components=3,\n",
    "                random_state=42,\n",
    "                init=\"pca\",\n",
    "                learning_rate=200,\n",
    "                perplexity=perplexity,\n",
    "                n_iter=3000,\n",
    "                early_exaggeration=12.0\n",
    "            )\n",
    "            coords_3d = tsne3.fit_transform(features)\n",
    "            method = \"t-SNE\"\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8), facecolor='white')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='white')\n",
    "\n",
    "        for i, cname in enumerate(CLASS_NAMES):\n",
    "            sel = (y_true_remapped == i)\n",
    "            ax.scatter(coords_3d[sel, 0], coords_3d[sel, 1], coords_3d[sel, 2],\n",
    "                       c=colors_3d[i], marker='o', label=cname,\n",
    "                       alpha=0.9, s=60, edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        ax.set_xlabel(f'{method} Component 1', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel(f'{method} Component 2', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_zlabel(f'{method} Component 3', fontsize=16, fontweight='bold', labelpad=15)\n",
    "\n",
    "        for axis in [ax.xaxis, ax.yaxis, ax.zaxis]:\n",
    "            axis.set_major_locator(plt.MaxNLocator(5))\n",
    "            for lab in axis.get_ticklabels():\n",
    "                lab.set_fontweight('bold')\n",
    "                lab.set_fontsize(11)\n",
    "\n",
    "        ax.legend(loc='upper right', fontsize=12, frameon=True,\n",
    "                  prop={'weight': 'bold'}, fancybox=True, shadow=True)\n",
    "        ax.grid(True, alpha=0.25, linestyle='--', linewidth=0.8, color='gray')\n",
    "\n",
    "        for pane in [ax.xaxis.pane, ax.yaxis.pane, ax.zaxis.pane]:\n",
    "            pane.fill = True\n",
    "            pane.set_facecolor('white')\n",
    "            pane.set_alpha(0.1)\n",
    "            pane.set_edgecolor('lightgray')\n",
    "\n",
    "        ax.view_init(elev=15, azim=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        png_filename = f\"{filename_prefix}_3D_{method}.png\"\n",
    "        pdf_filename = f\"{filename_prefix}_3D_{method}.pdf\"\n",
    "\n",
    "        plt.savefig(os.path.join(output_dir, png_filename), dpi=600, bbox_inches='tight', facecolor='white')\n",
    "        plt.savefig(os.path.join(output_dir, pdf_filename), bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {png_filename} and {pdf_filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_curves(history, output_dir, filename):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "        axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2.5, color='#1f77b4')\n",
    "        axes[0].plot(epochs, history['val_loss'], label='Validation', linewidth=2.5, color='#ff7f0e')\n",
    "        axes[0].set_xlabel('Epoch', fontsize=16, fontweight='bold')\n",
    "        axes[0].set_ylabel('Loss', fontsize=16, fontweight='bold')\n",
    "        axes[0].legend(fontsize=14, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[0].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[0].set_title('Training Loss', fontsize=16, fontweight='bold')\n",
    "        axes[0].tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        axes[1].plot(epochs, history['train_acc'], label='Train', linewidth=2.5, color='#1f77b4')\n",
    "        axes[1].plot(epochs, history['val_acc'], label='Validation', linewidth=2.5, color='#ff7f0e')\n",
    "        axes[1].set_xlabel('Epoch', fontsize=16, fontweight='bold')\n",
    "        axes[1].set_ylabel('Accuracy (%)', fontsize=16, fontweight='bold')\n",
    "        axes[1].legend(fontsize=14, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[1].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[1].set_title('Training Accuracy', fontsize=16, fontweight='bold')\n",
    "        axes[1].tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# DATA + FEATURES\n",
    "# -------------------------\n",
    "def robust_mad(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-12\n",
    "    return med, mad\n",
    "\n",
    "def find_1d_signal_in_mat(mat_dict):\n",
    "    candidates = []\n",
    "    for k, v in mat_dict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "            arr = np.array(v).squeeze()\n",
    "            if arr.ndim == 1 and arr.size > 1000:\n",
    "                candidates.append((k, arr.size, arr))\n",
    "    if not candidates:\n",
    "        for k, v in mat_dict.items():\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "                arr = np.array(v).squeeze()\n",
    "                if arr.size > 1000:\n",
    "                    return k, arr.reshape(-1).astype(np.float32)\n",
    "        raise ValueError(\"No suitable numeric signal array found in .mat file.\")\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0], candidates[0][2].astype(np.float32)\n",
    "\n",
    "def short_time_energy(x, win):\n",
    "    x2 = x.astype(np.float64) ** 2\n",
    "    kernel = np.ones(win, dtype=np.float64)\n",
    "    E = np.convolve(x2, kernel, mode=\"same\")\n",
    "    return E.astype(np.float32)\n",
    "\n",
    "def energy_peak_tokenize(x, energy_win=256, k_mad=6.0, peak_distance=800, seg_len=4096, max_tokens=200):\n",
    "    if x.size < seg_len + 10:\n",
    "        return []\n",
    "    E = short_time_energy(x, energy_win)\n",
    "    med, mad = robust_mad(E)\n",
    "    thr = med + k_mad * mad\n",
    "\n",
    "    peaks, props = find_peaks(E, height=thr, distance=peak_distance)\n",
    "    if peaks.size == 0:\n",
    "        mid = x.size // 2\n",
    "        half = seg_len // 2\n",
    "        seg = x[max(0, mid-half): min(x.size, mid+half)]\n",
    "        if seg.size == seg_len:\n",
    "            return [seg.astype(np.float32)]\n",
    "        return []\n",
    "\n",
    "    heights = props.get(\"peak_heights\", E[peaks])\n",
    "    order = np.argsort(heights)[::-1]\n",
    "    peaks = peaks[order][:max_tokens]\n",
    "\n",
    "    half = seg_len // 2\n",
    "    tokens = []\n",
    "    for p in peaks:\n",
    "        s = p - half\n",
    "        e = p + half\n",
    "        if s < 0 or e > x.size:\n",
    "            continue\n",
    "        seg = x[s:e].astype(np.float32)\n",
    "        if seg.size == seg_len:\n",
    "            tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "def time_features(seg):\n",
    "    x = seg.astype(np.float64)\n",
    "    x0 = x - np.mean(x)\n",
    "    rms = np.sqrt(np.mean(x0**2) + 1e-12)\n",
    "    peak = np.max(np.abs(x0)) + 1e-12\n",
    "    ptp = np.ptp(x0)\n",
    "    crest = peak / (rms + 1e-12)\n",
    "    kurt = kurtosis(x0, fisher=False, bias=False) if x0.size > 10 else 0.0\n",
    "    sk = skew(x0, bias=False) if x0.size > 10 else 0.0\n",
    "    return np.array([rms, peak, ptp, crest, kurt, sk], dtype=np.float32)\n",
    "\n",
    "def spectral_features(seg, fft_n=2048, eps=1e-12):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    n = min(len(x), fft_n)\n",
    "    w = np.hanning(n)\n",
    "    xw = x[:n] * w\n",
    "    X = np.fft.rfft(xw, n=n)\n",
    "    mag = np.abs(X) + eps\n",
    "    psd = mag**2\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)  # normalized bins\n",
    "\n",
    "    psd_sum = np.sum(psd) + eps\n",
    "    p = psd / psd_sum\n",
    "\n",
    "    centroid = np.sum(freqs * psd) / psd_sum\n",
    "    bandwidth = np.sqrt(np.sum(((freqs - centroid) ** 2) * psd) / psd_sum)\n",
    "    entropy = -np.sum(p * np.log(p + eps))\n",
    "    dom_idx = int(np.argmax(psd))\n",
    "    dom_freq = freqs[dom_idx]\n",
    "    rolloff_85 = freqs[np.searchsorted(np.cumsum(psd) / psd_sum, 0.85)]\n",
    "\n",
    "    return np.array([centroid, bandwidth, entropy, dom_freq, rolloff_85], dtype=np.float32)\n",
    "\n",
    "def wpt_topk_energy(seg, wavelet=\"db4\", level=5, topk=10):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet=wavelet, mode=\"symmetric\", maxlevel=level)\n",
    "    nodes = wp.get_level(level, order=\"freq\")\n",
    "    energies = np.array([np.sum(n.data**2) for n in nodes], dtype=np.float64)\n",
    "    energies = energies / (np.sum(energies) + 1e-12)\n",
    "    top = np.sort(energies)[::-1][:topk]\n",
    "    return top.astype(np.float32)\n",
    "\n",
    "def extract_features_from_token(seg):\n",
    "    tf = time_features(seg)\n",
    "    sf = spectral_features(seg, fft_n=FFT_N)\n",
    "    wf = wpt_topk_energy(seg, wavelet=WPT_WAVELET, level=WPT_LEVEL, topk=TOPK_WPT)\n",
    "    return np.concatenate([tf, sf, wf], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TORCH DATASET + MODEL\n",
    "# -------------------------\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, X, y, sid):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.sid = torch.tensor(sid, dtype=torch.long)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.sid[idx]\n",
    "\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, emb_dim)\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.fc1(x))\n",
    "        z = self.fc2(z)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "def compute_prototypes(model, loader, num_classes, device, emb_dim):\n",
    "    model.eval()\n",
    "    sums = torch.zeros((num_classes, emb_dim), device=device)\n",
    "    counts = torch.zeros((num_classes,), device=device)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            zb = model(xb)  # [B,D]\n",
    "            for c in range(num_classes):\n",
    "                mask = (yb == c)\n",
    "                if mask.any():\n",
    "                    sums[c] += zb[mask].sum(dim=0)\n",
    "                    counts[c] += mask.sum()\n",
    "    protos = sums / (counts.unsqueeze(1) + 1e-12)\n",
    "    protos = F.normalize(protos, dim=1)\n",
    "    return protos  # [C,D]\n",
    "\n",
    "def logits_from_prototypes(z, protos):\n",
    "    # cosine similarity as logits (since normalized)\n",
    "    return z @ protos.t()\n",
    "\n",
    "def accuracy_from_logits(logits, y):\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    return (pred == y).float().mean().item()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN EXPERIMENT FUNCTION\n",
    "# -------------------------\n",
    "def run_ept_ae_experiment(rpm_name, class_dirs, output_dir):\n",
    "    \"\"\"\n",
    "    Train + evaluate EPT-AE sample-level on a given RPM dataset.\n",
    "    Saves:\n",
    "      - reports (txt)\n",
    "      - arrays (npz)\n",
    "      - plots (png/pdf)\n",
    "    \"\"\"\n",
    "    global ORIGINAL_TO_NEW\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # ----- label mapping (in the order of class_dirs keys) -----\n",
    "    class_order = list(class_dirs.keys())\n",
    "    label_map = {cls:i for i, cls in enumerate(class_order)}\n",
    "    inv_label = {i:cls for cls,i in label_map.items()}\n",
    "\n",
    "    # Remap to BF,GF,TF,N order for plots\n",
    "    desired = CLASS_NAMES[:]  # [\"BF\",\"GF\",\"TF\",\"N\"]\n",
    "    ORIGINAL_TO_NEW = {label_map[c]: desired.index(c) for c in desired}\n",
    "\n",
    "    # ----- load -> tokenize -> features -----\n",
    "    all_X, all_y, all_sid = [], [], []\n",
    "    sample_meta = []  # (sid, class_name, filepath)\n",
    "    sid_counter = 0\n",
    "\n",
    "    for cls_name, folder in class_dirs.items():\n",
    "        mats = sorted(glob.glob(os.path.join(folder, \"*.mat\")))\n",
    "        if len(mats) == 0:\n",
    "            print(f\"[WARN] No .mat files found in: {folder}\")\n",
    "\n",
    "        for fp in mats:\n",
    "            try:\n",
    "                md = loadmat(fp)\n",
    "                key, sig = find_1d_signal_in_mat(md)\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] {fp} ({e})\")\n",
    "                continue\n",
    "\n",
    "            tokens = energy_peak_tokenize(\n",
    "                sig,\n",
    "                energy_win=ENERGY_WIN,\n",
    "                k_mad=K_MAD,\n",
    "                peak_distance=PEAK_DISTANCE,\n",
    "                seg_len=SEG_LEN,\n",
    "                max_tokens=MAX_TOKENS_PER_FILE\n",
    "            )\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "\n",
    "            sid = sid_counter\n",
    "            sid_counter += 1\n",
    "            sample_meta.append((sid, cls_name, fp))\n",
    "\n",
    "            for seg in tokens:\n",
    "                feat = extract_features_from_token(seg)\n",
    "                all_X.append(feat)\n",
    "                all_y.append(label_map[cls_name])\n",
    "                all_sid.append(sid)\n",
    "\n",
    "    all_X = np.stack(all_X, axis=0)\n",
    "    all_y = np.array(all_y, dtype=np.int64)\n",
    "    all_sid = np.array(all_sid, dtype=np.int64)\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Loaded tokens: {all_X.shape[0]} | Feature dim: {all_X.shape[1]}\")\n",
    "    token_counts = {inv_label[i]: int(np.sum(all_y == i)) for i in sorted(inv_label)}\n",
    "    print(f\"[{rpm_name}] Class counts (tokens): {token_counts}\")\n",
    "\n",
    "    # Save basic dataset summary\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_dataset_summary.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rpm\": rpm_name,\n",
    "            \"num_tokens\": int(all_X.shape[0]),\n",
    "            \"feature_dim\": int(all_X.shape[1]),\n",
    "            \"token_counts\": token_counts,\n",
    "            \"num_sample_files\": int(len(sample_meta))\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # ----- split BY SAMPLE FILE to avoid leakage -----\n",
    "    sample_ids = np.array([m[0] for m in sample_meta], dtype=np.int64)\n",
    "    sample_labels = np.array([label_map[m[1]] for m in sample_meta], dtype=np.int64)\n",
    "\n",
    "    train_sids, test_sids = train_test_split(\n",
    "        sample_ids, test_size=TEST_SIZE, random_state=SEED, stratify=sample_labels\n",
    "    )\n",
    "\n",
    "    # val split inside train (also by sample file)\n",
    "    train_sids, val_sids = train_test_split(\n",
    "        train_sids, test_size=VAL_SIZE_FROM_TRAIN, random_state=SEED,\n",
    "        stratify=sample_labels[np.isin(sample_ids, train_sids)]\n",
    "    )\n",
    "\n",
    "    train_mask = np.isin(all_sid, train_sids)\n",
    "    val_mask   = np.isin(all_sid, val_sids)\n",
    "    test_mask  = np.isin(all_sid, test_sids)\n",
    "\n",
    "    X_train, y_train, sid_train = all_X[train_mask], all_y[train_mask], all_sid[train_mask]\n",
    "    X_val,   y_val,   sid_val   = all_X[val_mask],   all_y[val_mask],   all_sid[val_mask]\n",
    "    X_test,  y_test,  sid_test  = all_X[test_mask],  all_y[test_mask],  all_sid[test_mask]\n",
    "\n",
    "    print(f\"[{rpm_name}] Train tokens: {X_train.shape[0]} | Val tokens: {X_val.shape[0]} | Test tokens: {X_test.shape[0]}\")\n",
    "    print(f\"[{rpm_name}] Train sample files: {len(train_sids)} | Val sample files: {len(val_sids)} | Test sample files: {len(test_sids)}\")\n",
    "\n",
    "    # ----- standardize -----\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "    X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    # Save scaler stats (for reproducibility)\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_scaler_stats.npz\"),\n",
    "             mean=scaler.mean_, scale=scaler.scale_)\n",
    "\n",
    "    # ----- loaders -----\n",
    "    train_ds = TokenDataset(X_train, y_train, sid_train)\n",
    "    val_ds   = TokenDataset(X_val,   y_val,   sid_val)\n",
    "    test_ds  = TokenDataset(X_test,  y_test,  sid_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    # ----- model -----\n",
    "    in_dim = X_train.shape[1]\n",
    "    num_classes = len(class_order)\n",
    "    emb_dim = 32\n",
    "\n",
    "    model = EmbNet(in_dim=in_dim, emb_dim=emb_dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # ----- training loop -----\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[]}\n",
    "\n",
    "    # Utility to compute epoch metrics\n",
    "    def eval_loss_acc(loader, protos):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_acc = 0.0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                z = model(xb)\n",
    "                logits = logits_from_prototypes(z, protos)\n",
    "                ce = F.cross_entropy(logits, yb)\n",
    "\n",
    "                p_y = protos[yb]\n",
    "                comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "                loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "                bs = xb.size(0)\n",
    "                total_loss += loss.item() * bs\n",
    "                total_acc  += (torch.argmax(logits, dim=1) == yb).float().sum().item()\n",
    "                n += bs\n",
    "        return total_loss / max(n,1), 100.0 * (total_acc / max(n,1))\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # compute prototypes from train set (non-episodic, not few-shot)\n",
    "        protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                    num_classes=num_classes, device=DEVICE, emb_dim=emb_dim)\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "            ce = F.cross_entropy(logits, yb)\n",
    "\n",
    "            p_y = protos[yb]\n",
    "            comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "\n",
    "            loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            running_correct += (torch.argmax(logits, dim=1) == yb).float().sum().item()\n",
    "            seen += bs\n",
    "\n",
    "        train_loss = running_loss / max(seen,1)\n",
    "        train_acc  = 100.0 * (running_correct / max(seen,1))\n",
    "\n",
    "        # validation metrics\n",
    "        val_loss, val_acc = eval_loss_acc(val_loader, protos)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0 or epoch == EPOCHS:\n",
    "            print(f\"[{rpm_name}] Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc:.2f}% | val loss {val_loss:.4f} acc {val_acc:.2f}%\")\n",
    "\n",
    "    # Save training curves\n",
    "    PublicationVisualizer.plot_training_curves(history, output_dir, f\"{rpm_name}_training_curves.png\")\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "\n",
    "    # ----- evaluation: TOKEN -> SAMPLE aggregation (weighted voting) -----\n",
    "    protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                num_classes=num_classes, device=DEVICE, emb_dim=emb_dim)\n",
    "    model.eval()\n",
    "\n",
    "    tok_true, tok_pred, tok_sid = [], [], []\n",
    "    tok_prob = []\n",
    "    tok_emb  = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, sidb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb)                       # [B,D]\n",
    "            logits = logits_from_prototypes(z, protos)  # [B,C]\n",
    "            probs = torch.softmax(ALPHA_VOTE * logits, dim=1)\n",
    "\n",
    "            pred = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "            tok_true.extend(yb.numpy().tolist())\n",
    "            tok_pred.extend(pred.tolist())\n",
    "            tok_sid.extend(sidb.numpy().tolist())\n",
    "            tok_prob.append(probs.cpu().numpy())\n",
    "            tok_emb.append(z.cpu().numpy())\n",
    "\n",
    "    tok_true = np.array(tok_true, dtype=np.int64)\n",
    "    tok_pred = np.array(tok_pred, dtype=np.int64)\n",
    "    tok_sid  = np.array(tok_sid,  dtype=np.int64)\n",
    "    tok_prob = np.concatenate(tok_prob, axis=0)  # [N_tokens, C]\n",
    "    tok_emb  = np.concatenate(tok_emb,  axis=0)  # [N_tokens, D]\n",
    "\n",
    "    # Build true label per sample\n",
    "    sample_true = {sid: label_map[cls] for sid, cls, _ in sample_meta if sid in set(test_sids)}\n",
    "\n",
    "    # Aggregate probabilities per sample\n",
    "    scores = {sid: np.zeros((num_classes,), dtype=np.float64) for sid in test_sids}\n",
    "    emb_sum = {sid: np.zeros((emb_dim,), dtype=np.float64) for sid in test_sids}\n",
    "    emb_cnt = {sid: 0 for sid in test_sids}\n",
    "\n",
    "    for sid, pvec, zvec in zip(tok_sid, tok_prob, tok_emb):\n",
    "        if sid in scores:\n",
    "            scores[sid] += pvec\n",
    "            emb_sum[sid] += zvec\n",
    "            emb_cnt[sid] += 1\n",
    "\n",
    "    sample_y_true, sample_y_pred, sample_y_proba = [], [], []\n",
    "    sample_emb = []\n",
    "    sample_ids_sorted = sorted(scores.keys())\n",
    "\n",
    "    for sid in sample_ids_sorted:\n",
    "        true = sample_true[sid]\n",
    "        sc = scores[sid]\n",
    "        proba = sc / (np.sum(sc) + 1e-12)\n",
    "        pred = int(np.argmax(sc))\n",
    "\n",
    "        sample_y_true.append(true)\n",
    "        sample_y_pred.append(pred)\n",
    "        sample_y_proba.append(proba)\n",
    "\n",
    "        if emb_cnt[sid] > 0:\n",
    "            sample_emb.append((emb_sum[sid] / emb_cnt[sid]).astype(np.float32))\n",
    "        else:\n",
    "            sample_emb.append(np.zeros((emb_dim,), dtype=np.float32))\n",
    "\n",
    "    sample_y_true = np.array(sample_y_true, dtype=np.int64)\n",
    "    sample_y_pred = np.array(sample_y_pred, dtype=np.int64)\n",
    "    sample_y_proba = np.stack(sample_y_proba, axis=0).astype(np.float32)\n",
    "    sample_emb = np.stack(sample_emb, axis=0).astype(np.float32)\n",
    "\n",
    "    # ----- save reports -----\n",
    "    report_text = classification_report(sample_y_true, sample_y_pred, target_names=class_order, digits=4)\n",
    "    cm = confusion_matrix(sample_y_true, sample_y_pred)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_report.txt\"), \"w\") as f:\n",
    "        f.write(f\"RPM: {rpm_name}\\n\")\n",
    "        f.write(\"Class order (internal): \" + str(class_order) + \"\\n\")\n",
    "        f.write(\"Desired order (plots): \" + str(CLASS_NAMES) + \"\\n\")\n",
    "        f.write(\"ORIGINAL_TO_NEW: \" + str(ORIGINAL_TO_NEW) + \"\\n\\n\")\n",
    "        f.write(report_text + \"\\n\\n\")\n",
    "        f.write(\"Confusion matrix (internal order):\\n\")\n",
    "        f.write(np.array2string(cm) + \"\\n\")\n",
    "\n",
    "    # Save arrays\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_outputs.npz\"),\n",
    "             sample_ids=np.array(sample_ids_sorted, dtype=np.int64),\n",
    "             y_true=sample_y_true,\n",
    "             y_pred=sample_y_pred,\n",
    "             y_proba=sample_y_proba,\n",
    "             emb=sample_emb)\n",
    "\n",
    "    # ----- plots -----\n",
    "    # Confusion matrix in BF/GF/TF/N order (your style)\n",
    "    PublicationVisualizer.plot_confusion_matrix(sample_y_true, sample_y_pred, output_dir, f\"{rpm_name}_CM_SAMPLE.png\")\n",
    "\n",
    "    # ROC using sample-level proba\n",
    "    PublicationVisualizer.plot_roc_curves(sample_y_true, sample_y_proba, output_dir, f\"{rpm_name}_ROC_SAMPLE.png\")\n",
    "\n",
    "    # t-SNE 2D/3D using sample embeddings (mean of token embeddings)\n",
    "    PublicationVisualizer.plot_tsne_2d(sample_emb, sample_y_true, output_dir, f\"{rpm_name}_tSNE2D_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_tsne_3d(sample_emb, sample_y_true, output_dir, f\"{rpm_name}_EMB\")\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] SAMPLE-level report:\\n{report_text}\")\n",
    "    print(f\"[{rpm_name}] Saved all outputs to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"rpm\": rpm_name,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"class_order\": class_order,\n",
    "        \"ORIGINAL_TO_NEW\": ORIGINAL_TO_NEW,\n",
    "        \"report\": report_text,\n",
    "        \"cm_internal\": cm\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (EDIT IF NEEDED)\n",
    "# -------------------------\n",
    "# 660 RPM\n",
    "BASE_660 = r\"F:\\20240925\"\n",
    "DIRS_660 = {\n",
    "    \"BF\": os.path.join(BASE_660, \"BF660_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_660, \"GF660_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_660, \"TF660_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_660, \"N660_1\",  \"AE\"),\n",
    "}\n",
    "OUT_660 = r\"E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\"\n",
    "\n",
    "# 720 RPM\n",
    "BASE_720 = r\"F:\\D4B2\\720\"\n",
    "DIRS_720 = {\n",
    "    \"BF\": os.path.join(BASE_720, \"BF720_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_720, \"GF720_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_720, \"TF720_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_720, \"N720_1\",  \"AE\"),\n",
    "}\n",
    "OUT_720 = r\"E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN BOTH EXPERIMENTS\n",
    "# -------------------------\n",
    "res_660 = run_ept_ae_experiment(\"660_RPM\", DIRS_660, OUT_660)\n",
    "res_720 = run_ept_ae_experiment(\"720_RPM\", DIRS_720, OUT_720)\n",
    "\n",
    "print(\"\\n✅ DONE for BOTH 660 & 720 RPM.\")\n",
    "print(\"660 outputs:\", OUT_660)\n",
    "print(\"720 outputs:\", OUT_720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0459ed23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[660_RPM] Loaded tokens: 115752 | Feature dim: 21\n",
      "[660_RPM] Class counts (tokens): {'BF': 23932, 'GF': 40187, 'TF': 25855, 'N': 25778}\n",
      "[660_RPM] Train tokens: 73505 | Val tokens: 13154 | Test tokens: 29093\n",
      "[660_RPM] Train sample files: 369 | Val sample files: 66 | Test sample files: 146\n",
      "[660_RPM] Epoch 01 | train loss 1.2724 acc 67.09% | val tok acc 74.35% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 05 | train loss 0.9076 acc 74.91% | val tok acc 78.39% | val SAMPLE acc 90.91% | best 100.00%\n",
      "[660_RPM] Epoch 10 | train loss 0.8647 acc 83.62% | val tok acc 85.05% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 15 | train loss 0.8584 acc 84.08% | val tok acc 85.24% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 20 | train loss 0.8523 acc 82.27% | val tok acc 84.63% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 25 | train loss 0.8471 acc 83.91% | val tok acc 83.88% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 30 | train loss 0.8476 acc 83.86% | val tok acc 84.65% | val SAMPLE acc 100.00% | best 100.00%\n",
      "    ✓ Saved: 660_RPM_training_curves.png\n",
      "    ✓ Saved: 660_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_EMB_3D_UMAP.png and 660_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[660_RPM] Best VAL SAMPLE acc = 100.00% at epoch 1 (majority, alpha=None)\n",
      "[660_RPM] TEST SAMPLE acc = 100.00% using majority alpha=None\n",
      "\n",
      "[660_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        30\n",
      "          GF     1.0000    1.0000    1.0000        50\n",
      "          TF     1.0000    1.0000    1.0000        33\n",
      "           N     1.0000    1.0000    1.0000        33\n",
      "\n",
      "    accuracy                         1.0000       146\n",
      "   macro avg     1.0000    1.0000    1.0000       146\n",
      "weighted avg     1.0000    1.0000    1.0000       146\n",
      "\n",
      "[660_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
      "\n",
      "[720_RPM] Loaded tokens: 133959 | Feature dim: 21\n",
      "[720_RPM] Class counts (tokens): {'BF': 30458, 'GF': 32960, 'TF': 36839, 'N': 33702}\n",
      "[720_RPM] Train tokens: 85332 | Val tokens: 15145 | Test tokens: 33482\n",
      "[720_RPM] Train sample files: 428 | Val sample files: 76 | Test sample files: 168\n",
      "[720_RPM] Epoch 01 | train loss 1.1466 acc 76.00% | val tok acc 80.48% | val SAMPLE acc 85.53% | best 85.53%\n",
      "[720_RPM] Epoch 05 | train loss 0.7419 acc 76.59% | val tok acc 74.49% | val SAMPLE acc 73.68% | best 85.53%\n",
      "[720_RPM] Epoch 10 | train loss 0.7278 acc 77.11% | val tok acc 76.36% | val SAMPLE acc 88.16% | best 94.74%\n",
      "[720_RPM] Epoch 15 | train loss 0.7262 acc 76.91% | val tok acc 76.27% | val SAMPLE acc 89.47% | best 94.74%\n",
      "[720_RPM] Epoch 20 | train loss 0.7246 acc 77.44% | val tok acc 76.82% | val SAMPLE acc 89.47% | best 94.74%\n",
      "[720_RPM] Epoch 25 | train loss 0.7239 acc 77.58% | val tok acc 77.78% | val SAMPLE acc 90.79% | best 94.74%\n",
      "[720_RPM] Epoch 30 | train loss 0.7222 acc 77.91% | val tok acc 77.81% | val SAMPLE acc 90.79% | best 94.74%\n",
      "    ✓ Saved: 720_RPM_training_curves.png\n",
      "    ✓ Saved: 720_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_EMB_3D_UMAP.png and 720_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[720_RPM] Best VAL SAMPLE acc = 94.74% at epoch 6 (weighted, alpha=5.0)\n",
      "[720_RPM] TEST SAMPLE acc = 92.26% using weighted alpha=5.0\n",
      "\n",
      "[720_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        38\n",
      "          GF     0.7692    0.9756    0.8602        41\n",
      "          TF     1.0000    1.0000    1.0000        47\n",
      "           N     0.9677    0.7143    0.8219        42\n",
      "\n",
      "    accuracy                         0.9226       168\n",
      "   macro avg     0.9342    0.9225    0.9205       168\n",
      "weighted avg     0.9356    0.9226    0.9214       168\n",
      "\n",
      "[720_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n",
      "\n",
      "✅ DONE for BOTH 660 & 720 RPM.\n",
      "660 outputs: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
      "720 outputs: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EPT-AE (Energy–Peak Tokenization + Regularized Prototype Classifier)\n",
    "# FULL UPDATED END-TO-END CODE (ONE CELL) for BOTH 660 & 720 RPM\n",
    "#\n",
    "# Improvements included:\n",
    "#  1) Strict file-level split (train/val/test by sample files) -> no leakage\n",
    "#  2) Best checkpoint selection based on VAL SAMPLE-LEVEL accuracy (not token-level)\n",
    "#  3) Auto-select best aggregation on VAL between:\n",
    "#       - Majority vote\n",
    "#       - Weighted vote with alpha grid\n",
    "#     Then apply best choice to TEST\n",
    "#  4) Save all results + publication plots (CM, ROC, t-SNE 2D/3D, training curves)\n",
    "#  5) Save arrays/reports/history/best-selection json to:\n",
    "#       660 -> E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
    "#       720 -> E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, json\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import pywt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# -------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Tokenization\n",
    "ENERGY_WIN = 256\n",
    "PEAK_DISTANCE = 800\n",
    "K_MAD = 6.0\n",
    "\n",
    "SEG_LEN = 4096\n",
    "MAX_TOKENS_PER_FILE = 200\n",
    "\n",
    "# Feature extraction\n",
    "WPT_WAVELET = \"db4\"\n",
    "WPT_LEVEL = 5\n",
    "TOPK_WPT = 10\n",
    "FFT_N = 2048\n",
    "\n",
    "# Splits\n",
    "TEST_SIZE = 0.25\n",
    "VAL_SIZE_FROM_TRAIN = 0.15\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "LAMBDA_COMPACT = 0.2\n",
    "\n",
    "# Aggregation tuning (for weighted vote)\n",
    "ALPHA_GRID = (5.0, 10.0, 15.0, 20.0)\n",
    "\n",
    "# Visualization\n",
    "CLASS_NAMES = [\"BF\", \"GF\", \"TF\", \"N\"]  # desired order for plots\n",
    "ORIGINAL_TO_NEW = {0:0, 1:1, 2:2, 3:3}  # will be set per experiment\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# REPRODUCIBILITY\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VISUALIZATION CLASS (publication-ready)\n",
    "# -------------------------\n",
    "class PublicationVisualizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def remap_labels(labels):\n",
    "        \"\"\"Remap labels to new order: BF(0), GF(1), TF(2), N(3)\"\"\"\n",
    "        return np.array([ORIGINAL_TO_NEW[int(label)] for label in labels])\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, output_dir, filename):\n",
    "        \"\"\"Confusion matrix with BF, GF, TF, N order\"\"\"\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "        y_pred_remapped = PublicationVisualizer.remap_labels(y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_true_remapped, y_pred_remapped)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                    cbar=False, annot_kws={\"size\": 22, \"fontweight\": \"bold\"})\n",
    "        plt.xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "        plt.setp(plt.gca().get_xticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.setp(plt.gca().get_yticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(y_true, y_proba, output_dir, filename):\n",
    "        \"\"\"ROC curves with BF, GF, TF, N order\"\"\"\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        # Remap probabilities\n",
    "        y_proba_remapped = np.zeros_like(y_proba)\n",
    "        for old_idx, new_idx in ORIGINAL_TO_NEW.items():\n",
    "            y_proba_remapped[:, new_idx] = y_proba[:, old_idx]\n",
    "\n",
    "        y_bin = label_binarize(y_true_remapped, classes=[0, 1, 2, 3])\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "        for i in range(4):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba_remapped[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2.5, color=colors[i], linestyle=line_styles[i],\n",
    "                     label=f'{CLASS_NAMES[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=13, frameon=True, framealpha=0.95)\n",
    "        plt.grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.xlim([-0.02, 1.02])\n",
    "        plt.ylim([-0.02, 1.02])\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_2d(features, y_true, output_dir, filename):\n",
    "        \"\"\"2D t-SNE with BF, GF, TF, N order\"\"\"\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        markers = ['o', 's', '^', 'D']\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "        tsne = TSNE(\n",
    "            n_components=2,\n",
    "            random_state=42,\n",
    "            init='pca',\n",
    "            learning_rate=200,\n",
    "            perplexity=perplexity,\n",
    "            n_iter=3000,\n",
    "            early_exaggeration=12.0,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        features_2d = tsne.fit_transform(features)\n",
    "\n",
    "        plt.figure(figsize=(8, 7))\n",
    "        for i, (cname, m, col) in enumerate(zip(CLASS_NAMES, markers, colors)):\n",
    "            sel = (y_true_remapped == i)\n",
    "            plt.scatter(features_2d[sel, 0], features_2d[sel, 1],\n",
    "                        marker=m, color=col, label=cname, alpha=0.85, s=80,\n",
    "                        edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        plt.legend(title=\"Fault Types\", loc='best',\n",
    "                   prop={'weight': 'bold', 'size': 14}, title_fontsize=15,\n",
    "                   frameon=True, fancybox=True, shadow=True)\n",
    "        plt.xlabel('t-SNE Component 1', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('t-SNE Component 2', fontsize=18, fontweight='bold')\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.grid(alpha=0.2, linestyle='--', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_3d(features, y_true, output_dir, filename_prefix):\n",
    "        \"\"\"3D UMAP (preferred) or 3D t-SNE with BF, GF, TF, N order\"\"\"\n",
    "        y_true_remapped = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        rcParams['font.family'] = 'Arial'\n",
    "        rcParams['font.size'] = 12\n",
    "        colors_3d = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        coords_3d = None\n",
    "        method = None\n",
    "        try:\n",
    "            import umap.umap_ as umap\n",
    "            reducer = umap.UMAP(\n",
    "                n_components=3,\n",
    "                n_neighbors=15,\n",
    "                min_dist=0.3,\n",
    "                metric=\"euclidean\",\n",
    "                random_state=42,\n",
    "                spread=1.5\n",
    "            )\n",
    "            coords_3d = reducer.fit_transform(features)\n",
    "            method = \"UMAP\"\n",
    "        except Exception:\n",
    "            perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "            tsne3 = TSNE(\n",
    "                n_components=3,\n",
    "                random_state=42,\n",
    "                init=\"pca\",\n",
    "                learning_rate=200,\n",
    "                perplexity=perplexity,\n",
    "                n_iter=3000,\n",
    "                early_exaggeration=12.0\n",
    "            )\n",
    "            coords_3d = tsne3.fit_transform(features)\n",
    "            method = \"t-SNE\"\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8), facecolor='white')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='white')\n",
    "\n",
    "        for i, cname in enumerate(CLASS_NAMES):\n",
    "            sel = (y_true_remapped == i)\n",
    "            ax.scatter(coords_3d[sel, 0], coords_3d[sel, 1], coords_3d[sel, 2],\n",
    "                       c=colors_3d[i], marker='o', label=cname,\n",
    "                       alpha=0.9, s=60, edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        ax.set_xlabel(f'{method} Component 1', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel(f'{method} Component 2', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_zlabel(f'{method} Component 3', fontsize=16, fontweight='bold', labelpad=15)\n",
    "\n",
    "        for axis in [ax.xaxis, ax.yaxis, ax.zaxis]:\n",
    "            axis.set_major_locator(plt.MaxNLocator(5))\n",
    "            for lab in axis.get_ticklabels():\n",
    "                lab.set_fontweight('bold')\n",
    "                lab.set_fontsize(11)\n",
    "\n",
    "        ax.legend(loc='upper right', fontsize=12, frameon=True,\n",
    "                  prop={'weight': 'bold'}, fancybox=True, shadow=True)\n",
    "        ax.grid(True, alpha=0.25, linestyle='--', linewidth=0.8, color='gray')\n",
    "\n",
    "        for pane in [ax.xaxis.pane, ax.yaxis.pane, ax.zaxis.pane]:\n",
    "            pane.fill = True\n",
    "            pane.set_facecolor('white')\n",
    "            pane.set_alpha(0.1)\n",
    "            pane.set_edgecolor('lightgray')\n",
    "\n",
    "        ax.view_init(elev=15, azim=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        png_filename = f\"{filename_prefix}_3D_{method}.png\"\n",
    "        pdf_filename = f\"{filename_prefix}_3D_{method}.pdf\"\n",
    "\n",
    "        plt.savefig(os.path.join(output_dir, png_filename), dpi=600, bbox_inches='tight', facecolor='white')\n",
    "        plt.savefig(os.path.join(output_dir, pdf_filename), bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {png_filename} and {pdf_filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_curves(history, output_dir, filename):\n",
    "        \"\"\"Training curves (loss + accuracy + sample-level val acc)\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "        axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2.5)\n",
    "        axes[0].plot(epochs, history['val_loss'], label='Validation', linewidth=2.5)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[0].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[0].set_title('Token-level Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "        axes[1].plot(epochs, history['train_acc'], label='Train', linewidth=2.5)\n",
    "        axes[1].plot(epochs, history['val_acc'], label='Validation', linewidth=2.5)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[1].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[1].set_title('Token-level Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "        axes[2].plot(epochs, history['val_sample_acc'], label='Val SAMPLE', linewidth=2.5)\n",
    "        axes[2].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "        axes[2].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[2].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[2].set_title('Validation SAMPLE-level Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SIGNAL UTILS + FEATURES\n",
    "# -------------------------\n",
    "def robust_mad(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-12\n",
    "    return med, mad\n",
    "\n",
    "def find_1d_signal_in_mat(mat_dict):\n",
    "    \"\"\"Pick the first large 1D numeric array in .mat file (robust heuristic).\"\"\"\n",
    "    candidates = []\n",
    "    for k, v in mat_dict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "            arr = np.array(v).squeeze()\n",
    "            if arr.ndim == 1 and arr.size > 1000:\n",
    "                candidates.append((k, arr.size, arr))\n",
    "    if not candidates:\n",
    "        for k, v in mat_dict.items():\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "                arr = np.array(v).squeeze()\n",
    "                if arr.size > 1000:\n",
    "                    return k, arr.reshape(-1).astype(np.float32)\n",
    "        raise ValueError(\"No suitable numeric signal array found in .mat file.\")\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0], candidates[0][2].astype(np.float32)\n",
    "\n",
    "def short_time_energy(x, win):\n",
    "    x2 = x.astype(np.float64) ** 2\n",
    "    kernel = np.ones(win, dtype=np.float64)\n",
    "    E = np.convolve(x2, kernel, mode=\"same\")\n",
    "    return E.astype(np.float32)\n",
    "\n",
    "def energy_peak_tokenize(x, energy_win=256, k_mad=6.0, peak_distance=800, seg_len=4096, max_tokens=200):\n",
    "    \"\"\"Return list of AE event tokens centered around energy peaks.\"\"\"\n",
    "    if x.size < seg_len + 10:\n",
    "        return []\n",
    "    E = short_time_energy(x, energy_win)\n",
    "    med, mad = robust_mad(E)\n",
    "    thr = med + k_mad * mad\n",
    "\n",
    "    peaks, props = find_peaks(E, height=thr, distance=peak_distance)\n",
    "    if peaks.size == 0:\n",
    "        mid = x.size // 2\n",
    "        half = seg_len // 2\n",
    "        seg = x[max(0, mid-half): min(x.size, mid+half)]\n",
    "        return [seg.astype(np.float32)] if seg.size == seg_len else []\n",
    "\n",
    "    heights = props.get(\"peak_heights\", E[peaks])\n",
    "    order = np.argsort(heights)[::-1]\n",
    "    peaks = peaks[order][:max_tokens]\n",
    "\n",
    "    half = seg_len // 2\n",
    "    tokens = []\n",
    "    for p in peaks:\n",
    "        s = p - half\n",
    "        e = p + half\n",
    "        if s < 0 or e > x.size:\n",
    "            continue\n",
    "        seg = x[s:e].astype(np.float32)\n",
    "        if seg.size == seg_len:\n",
    "            tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "def time_features(seg):\n",
    "    x = seg.astype(np.float64)\n",
    "    x0 = x - np.mean(x)\n",
    "    rms = np.sqrt(np.mean(x0**2) + 1e-12)\n",
    "    peak = np.max(np.abs(x0)) + 1e-12\n",
    "    ptp = np.ptp(x0)\n",
    "    crest = peak / (rms + 1e-12)\n",
    "    kurt = kurtosis(x0, fisher=False, bias=False) if x0.size > 10 else 0.0\n",
    "    sk = skew(x0, bias=False) if x0.size > 10 else 0.0\n",
    "    return np.array([rms, peak, ptp, crest, kurt, sk], dtype=np.float32)\n",
    "\n",
    "def spectral_features(seg, fft_n=2048, eps=1e-12):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    n = min(len(x), fft_n)\n",
    "    w = np.hanning(n)\n",
    "    xw = x[:n] * w\n",
    "    X = np.fft.rfft(xw, n=n)\n",
    "    mag = np.abs(X) + eps\n",
    "    psd = mag**2\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)  # normalized bins\n",
    "\n",
    "    psd_sum = np.sum(psd) + eps\n",
    "    p = psd / psd_sum\n",
    "\n",
    "    centroid = np.sum(freqs * psd) / psd_sum\n",
    "    bandwidth = np.sqrt(np.sum(((freqs - centroid) ** 2) * psd) / psd_sum)\n",
    "    entropy = -np.sum(p * np.log(p + eps))\n",
    "    dom_idx = int(np.argmax(psd))\n",
    "    dom_freq = freqs[dom_idx]\n",
    "    rolloff_85 = freqs[np.searchsorted(np.cumsum(psd) / psd_sum, 0.85)]\n",
    "\n",
    "    return np.array([centroid, bandwidth, entropy, dom_freq, rolloff_85], dtype=np.float32)\n",
    "\n",
    "def wpt_topk_energy(seg, wavelet=\"db4\", level=5, topk=10):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet=wavelet, mode=\"symmetric\", maxlevel=level)\n",
    "    nodes = wp.get_level(level, order=\"freq\")\n",
    "    energies = np.array([np.sum(n.data**2) for n in nodes], dtype=np.float64)\n",
    "    energies = energies / (np.sum(energies) + 1e-12)\n",
    "    top = np.sort(energies)[::-1][:topk]\n",
    "    return top.astype(np.float32)\n",
    "\n",
    "def extract_features_from_token(seg):\n",
    "    tf = time_features(seg)\n",
    "    sf = spectral_features(seg, fft_n=FFT_N)\n",
    "    wf = wpt_topk_energy(seg, wavelet=WPT_WAVELET, level=WPT_LEVEL, topk=TOPK_WPT)\n",
    "    return np.concatenate([tf, sf, wf], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TORCH DATA + MODEL\n",
    "# -------------------------\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, X, y, sid):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.sid = torch.tensor(sid, dtype=torch.long)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.sid[idx]\n",
    "\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, emb_dim)\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.fc1(x))\n",
    "        z = self.fc2(z)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "def compute_prototypes(model, loader, num_classes, device, emb_dim):\n",
    "    model.eval()\n",
    "    sums = torch.zeros((num_classes, emb_dim), device=device)\n",
    "    counts = torch.zeros((num_classes,), device=device)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            zb = model(xb)\n",
    "            for c in range(num_classes):\n",
    "                m = (yb == c)\n",
    "                if m.any():\n",
    "                    sums[c] += zb[m].sum(dim=0)\n",
    "                    counts[c] += m.sum()\n",
    "    protos = sums / (counts.unsqueeze(1) + 1e-12)\n",
    "    protos = F.normalize(protos, dim=1)\n",
    "    return protos\n",
    "\n",
    "def logits_from_prototypes(z, protos):\n",
    "    return z @ protos.t()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAMPLE-LEVEL AGGREGATION (majority + weighted, choose best)\n",
    "# -------------------------\n",
    "def sample_level_eval(model, loader, sids_set, sample_true_dict, protos, num_classes, alphas):\n",
    "    model.eval()\n",
    "\n",
    "    tok_sid = []\n",
    "    tok_pred_major = []\n",
    "    tok_prob_by_alpha = {a: [] for a in alphas}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, sidb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            # majority uses argmax logits\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            tok_pred_major.extend(pred.tolist())\n",
    "\n",
    "            tok_sid.extend(sidb.numpy().tolist())\n",
    "\n",
    "            # weighted uses probs for each alpha\n",
    "            for a in alphas:\n",
    "                probs = torch.softmax(a * logits, dim=1).cpu().numpy()\n",
    "                tok_prob_by_alpha[a].append(probs)\n",
    "\n",
    "    tok_sid = np.array(tok_sid, dtype=np.int64)\n",
    "    tok_pred_major = np.array(tok_pred_major, dtype=np.int64)\n",
    "\n",
    "    # ---- majority aggregation ----\n",
    "    votes = {sid: [] for sid in sids_set}\n",
    "    for sid, p in zip(tok_sid, tok_pred_major):\n",
    "        if sid in votes:\n",
    "            votes[sid].append(p)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_major = []\n",
    "    for sid in sorted(votes.keys()):\n",
    "        y_true.append(sample_true_dict[sid])\n",
    "        if len(votes[sid]) == 0:\n",
    "            y_pred_major.append(0)  # fallback BF (rare). you can change to N if you want\n",
    "        else:\n",
    "            vals, cnts = np.unique(votes[sid], return_counts=True)\n",
    "            y_pred_major.append(int(vals[np.argmax(cnts)]))\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.int64)\n",
    "    y_pred_major = np.array(y_pred_major, dtype=np.int64)\n",
    "    acc_major = float((y_true == y_pred_major).mean())\n",
    "\n",
    "    # create proba for ROC when using majority (one-hot)\n",
    "    proba_major = np.zeros((len(y_pred_major), num_classes), dtype=np.float32)\n",
    "    for i,p in enumerate(y_pred_major):\n",
    "        proba_major[i,p] = 1.0\n",
    "\n",
    "    best = {\"mode\":\"majority\", \"alpha\":None, \"acc\":acc_major,\n",
    "            \"y_true\":y_true, \"y_pred\":y_pred_major, \"y_proba\":proba_major}\n",
    "\n",
    "    # ---- weighted aggregation: try multiple alphas and pick best ----\n",
    "    for a in alphas:\n",
    "        probs_all = np.concatenate(tok_prob_by_alpha[a], axis=0)  # [Ntok,C]\n",
    "        scores = {sid: np.zeros((num_classes,), dtype=np.float64) for sid in sids_set}\n",
    "\n",
    "        for sid, pv in zip(tok_sid, probs_all):\n",
    "            if sid in scores:\n",
    "                scores[sid] += pv\n",
    "\n",
    "        y_pred = []\n",
    "        y_proba = []\n",
    "        for sid in sorted(scores.keys()):\n",
    "            sc = scores[sid]\n",
    "            y_pred.append(int(np.argmax(sc)))\n",
    "            y_proba.append((sc / (np.sum(sc) + 1e-12)).astype(np.float32))\n",
    "\n",
    "        y_pred = np.array(y_pred, dtype=np.int64)\n",
    "        acc = float((y_true == y_pred).mean())\n",
    "\n",
    "        if acc > best[\"acc\"]:\n",
    "            best = {\"mode\":\"weighted\", \"alpha\":float(a), \"acc\":acc,\n",
    "                    \"y_true\":y_true, \"y_pred\":y_pred, \"y_proba\":np.stack(y_proba, axis=0)}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN EXPERIMENT (with best checkpoint via VAL SAMPLE-level)\n",
    "# -------------------------\n",
    "def run_ept_ae_experiment(rpm_name, class_dirs, output_dir):\n",
    "    global ORIGINAL_TO_NEW\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    class_order = list(class_dirs.keys())\n",
    "    label_map = {cls:i for i, cls in enumerate(class_order)}\n",
    "    inv_label = {i:cls for cls,i in label_map.items()}\n",
    "\n",
    "    # Set remap dict so plots always show BF,GF,TF,N\n",
    "    ORIGINAL_TO_NEW = {label_map[c]: CLASS_NAMES.index(c) for c in CLASS_NAMES}\n",
    "\n",
    "    # -------- Load -> Tokenize -> Features --------\n",
    "    all_X, all_y, all_sid = [], [], []\n",
    "    sample_meta = []  # (sid, class_name, filepath)\n",
    "    sid_counter = 0\n",
    "\n",
    "    for cls_name, folder in class_dirs.items():\n",
    "        mats = sorted(glob.glob(os.path.join(folder, \"*.mat\")))\n",
    "        if len(mats) == 0:\n",
    "            print(f\"[WARN] No .mat files found in: {folder}\")\n",
    "\n",
    "        for fp in mats:\n",
    "            try:\n",
    "                md = loadmat(fp)\n",
    "                _, sig = find_1d_signal_in_mat(md)\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] {fp} ({e})\")\n",
    "                continue\n",
    "\n",
    "            tokens = energy_peak_tokenize(\n",
    "                sig, energy_win=ENERGY_WIN, k_mad=K_MAD,\n",
    "                peak_distance=PEAK_DISTANCE, seg_len=SEG_LEN,\n",
    "                max_tokens=MAX_TOKENS_PER_FILE\n",
    "            )\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "\n",
    "            sid = sid_counter\n",
    "            sid_counter += 1\n",
    "            sample_meta.append((sid, cls_name, fp))\n",
    "\n",
    "            for seg in tokens:\n",
    "                all_X.append(extract_features_from_token(seg))\n",
    "                all_y.append(label_map[cls_name])\n",
    "                all_sid.append(sid)\n",
    "\n",
    "    all_X = np.stack(all_X, axis=0)\n",
    "    all_y = np.array(all_y, dtype=np.int64)\n",
    "    all_sid = np.array(all_sid, dtype=np.int64)\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Loaded tokens: {all_X.shape[0]} | Feature dim: {all_X.shape[1]}\")\n",
    "    token_counts = {inv_label[i]: int(np.sum(all_y == i)) for i in sorted(inv_label)}\n",
    "    print(f\"[{rpm_name}] Class counts (tokens): {token_counts}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_dataset_summary.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rpm\": rpm_name,\n",
    "            \"num_tokens\": int(all_X.shape[0]),\n",
    "            \"feature_dim\": int(all_X.shape[1]),\n",
    "            \"token_counts\": token_counts,\n",
    "            \"num_sample_files\": int(len(sample_meta)),\n",
    "            \"ORIGINAL_TO_NEW\": ORIGINAL_TO_NEW\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # -------- Split by SAMPLE FILE (no leakage) --------\n",
    "    sample_ids = np.array([m[0] for m in sample_meta], dtype=np.int64)\n",
    "    sample_labels = np.array([label_map[m[1]] for m in sample_meta], dtype=np.int64)\n",
    "\n",
    "    train_sids, test_sids = train_test_split(\n",
    "        sample_ids, test_size=TEST_SIZE, random_state=SEED, stratify=sample_labels\n",
    "    )\n",
    "\n",
    "    train_labels = sample_labels[np.isin(sample_ids, train_sids)]\n",
    "    train_sids, val_sids = train_test_split(\n",
    "        train_sids, test_size=VAL_SIZE_FROM_TRAIN, random_state=SEED, stratify=train_labels\n",
    "    )\n",
    "\n",
    "    train_mask = np.isin(all_sid, train_sids)\n",
    "    val_mask   = np.isin(all_sid, val_sids)\n",
    "    test_mask  = np.isin(all_sid, test_sids)\n",
    "\n",
    "    X_train, y_train, sid_train = all_X[train_mask], all_y[train_mask], all_sid[train_mask]\n",
    "    X_val,   y_val,   sid_val   = all_X[val_mask],   all_y[val_mask],   all_sid[val_mask]\n",
    "    X_test,  y_test,  sid_test  = all_X[test_mask],  all_y[test_mask],  all_sid[test_mask]\n",
    "\n",
    "    print(f\"[{rpm_name}] Train tokens: {X_train.shape[0]} | Val tokens: {X_val.shape[0]} | Test tokens: {X_test.shape[0]}\")\n",
    "    print(f\"[{rpm_name}] Train sample files: {len(train_sids)} | Val sample files: {len(val_sids)} | Test sample files: {len(test_sids)}\")\n",
    "\n",
    "    # -------- Standardize --------\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "    X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_scaler_stats.npz\"),\n",
    "             mean=scaler.mean_, scale=scaler.scale_)\n",
    "\n",
    "    # -------- Loaders --------\n",
    "    train_ds = TokenDataset(X_train, y_train, sid_train)\n",
    "    val_ds   = TokenDataset(X_val,   y_val,   sid_val)\n",
    "    test_ds  = TokenDataset(X_test,  y_test,  sid_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    # -------- Model --------\n",
    "    in_dim = X_train.shape[1]\n",
    "    num_classes = len(class_order)\n",
    "    emb_dim = 32\n",
    "\n",
    "    model = EmbNet(in_dim=in_dim, emb_dim=emb_dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # sample_true dicts for val/test\n",
    "    meta_dict = {sid: cls for sid, cls, _ in sample_meta}\n",
    "    val_true  = {sid: label_map[meta_dict[sid]] for sid in val_sids}\n",
    "    test_true = {sid: label_map[meta_dict[sid]] for sid in test_sids}\n",
    "\n",
    "    # -------- Training with BEST checkpoint by VAL SAMPLE acc --------\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[], \"val_sample_acc\":[]}\n",
    "    best_state = None\n",
    "    best_val_sample_acc = -1.0\n",
    "    best_choice = None\n",
    "\n",
    "    def token_eval_loss_acc(loader, protos):\n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "                z = model(xb)\n",
    "                logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "                ce = F.cross_entropy(logits, yb)\n",
    "                p_y = protos[yb]\n",
    "                comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "                loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "                bs = xb.size(0)\n",
    "                total_loss += loss.item() * bs\n",
    "                total_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "                n += bs\n",
    "        return total_loss/max(n,1), 100.0*(total_correct/max(n,1))\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        # prototypes from train\n",
    "        protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                    num_classes=num_classes, device=DEVICE, emb_dim=emb_dim)\n",
    "\n",
    "        model.train()\n",
    "        run_loss = 0.0\n",
    "        run_correct = 0\n",
    "        seen = 0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            ce = F.cross_entropy(logits, yb)\n",
    "            p_y = protos[yb]\n",
    "            comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "            loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            run_loss += loss.item() * bs\n",
    "            run_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "            seen += bs\n",
    "\n",
    "        train_loss = run_loss/max(seen,1)\n",
    "        train_acc = 100.0*(run_correct/max(seen,1))\n",
    "\n",
    "        val_loss, val_acc = token_eval_loss_acc(val_loader, protos)\n",
    "\n",
    "        # ✅ val sample-level selection\n",
    "        val_choice = sample_level_eval(model, val_loader, set(val_sids), val_true,\n",
    "                                       protos, num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "        val_sample_acc = 100.0 * val_choice[\"acc\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_sample_acc\"].append(val_sample_acc)\n",
    "\n",
    "        if val_choice[\"acc\"] > best_val_sample_acc:\n",
    "            best_val_sample_acc = val_choice[\"acc\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            best_choice = {\"epoch\": epoch, \"mode\": val_choice[\"mode\"], \"alpha\": val_choice[\"alpha\"],\n",
    "                           \"val_sample_acc\": float(val_sample_acc)}\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0 or epoch == EPOCHS:\n",
    "            print(f\"[{rpm_name}] Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc:.2f}% | \"\n",
    "                  f\"val tok acc {val_acc:.2f}% | val SAMPLE acc {val_sample_acc:.2f}% | best {best_val_sample_acc*100:.2f}%\")\n",
    "\n",
    "    # restore best\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                num_classes=num_classes, device=DEVICE, emb_dim=emb_dim)\n",
    "\n",
    "    # Save history + best selection\n",
    "    PublicationVisualizer.plot_training_curves(history, output_dir, f\"{rpm_name}_training_curves.png\")\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_best_selection.json\"), \"w\") as f:\n",
    "        json.dump(best_choice, f, indent=2)\n",
    "\n",
    "    # -------- TEST sample-level with auto-aggregation --------\n",
    "    test_choice = sample_level_eval(model, test_loader, set(test_sids), test_true,\n",
    "                                    protos, num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "\n",
    "    y_true_s = test_choice[\"y_true\"]\n",
    "    y_pred_s = test_choice[\"y_pred\"]\n",
    "    y_proba_s = test_choice[\"y_proba\"]\n",
    "\n",
    "    report_text = classification_report(y_true_s, y_pred_s, target_names=class_order, digits=4)\n",
    "    cm = confusion_matrix(y_true_s, y_pred_s)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_report.txt\"), \"w\") as f:\n",
    "        f.write(f\"RPM: {rpm_name}\\n\")\n",
    "        f.write(\"Internal class order: \" + str(class_order) + \"\\n\")\n",
    "        f.write(\"Plot class order: \" + str(CLASS_NAMES) + \"\\n\")\n",
    "        f.write(\"ORIGINAL_TO_NEW: \" + str(ORIGINAL_TO_NEW) + \"\\n\")\n",
    "        f.write(\"Best checkpoint (VAL SAMPLE): \" + str(best_choice) + \"\\n\")\n",
    "        f.write(\"Test aggregation: \" + str({k:test_choice[k] for k in ['mode','alpha','acc']}) + \"\\n\\n\")\n",
    "        f.write(report_text + \"\\n\\n\")\n",
    "        f.write(\"Confusion matrix (internal order):\\n\")\n",
    "        f.write(np.array2string(cm) + \"\\n\")\n",
    "\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_outputs.npz\"),\n",
    "             y_true=y_true_s, y_pred=y_pred_s, y_proba=y_proba_s)\n",
    "\n",
    "    # Plots\n",
    "    PublicationVisualizer.plot_confusion_matrix(y_true_s, y_pred_s, output_dir, f\"{rpm_name}_CM_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_roc_curves(y_true_s, y_proba_s, output_dir, f\"{rpm_name}_ROC_SAMPLE.png\")\n",
    "\n",
    "    # For t-SNE, compute sample embeddings as mean token embedding per file (from TEST)\n",
    "    # Collect token embeddings and average per sid\n",
    "    model.eval()\n",
    "    emb_dim = protos.shape[1]\n",
    "    emb_sum = {sid: np.zeros((emb_dim,), dtype=np.float64) for sid in test_sids}\n",
    "    emb_cnt = {sid: 0 for sid in test_sids}\n",
    "    with torch.no_grad():\n",
    "        for xb, _, sidb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb).cpu().numpy()\n",
    "            sids = sidb.numpy()\n",
    "            for zi, si in zip(z, sids):\n",
    "                if int(si) in emb_sum:\n",
    "                    emb_sum[int(si)] += zi\n",
    "                    emb_cnt[int(si)] += 1\n",
    "\n",
    "    sample_ids_sorted = sorted(list(test_sids))\n",
    "    sample_emb = []\n",
    "    sample_y_for_tsne = []\n",
    "    for sid in sample_ids_sorted:\n",
    "        if emb_cnt[sid] > 0:\n",
    "            sample_emb.append((emb_sum[sid] / emb_cnt[sid]).astype(np.float32))\n",
    "        else:\n",
    "            sample_emb.append(np.zeros((emb_dim,), dtype=np.float32))\n",
    "        sample_y_for_tsne.append(test_true[sid])\n",
    "\n",
    "    sample_emb = np.stack(sample_emb, axis=0)\n",
    "    sample_y_for_tsne = np.array(sample_y_for_tsne, dtype=np.int64)\n",
    "\n",
    "    PublicationVisualizer.plot_tsne_2d(sample_emb, sample_y_for_tsne, output_dir, f\"{rpm_name}_tSNE2D_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_tsne_3d(sample_emb, sample_y_for_tsne, output_dir, f\"{rpm_name}_EMB\")\n",
    "\n",
    "    # Save embeddings for later reuse\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_embeddings.npz\"),\n",
    "             sample_ids=np.array(sample_ids_sorted, dtype=np.int64),\n",
    "             emb=sample_emb,\n",
    "             y_true=sample_y_for_tsne)\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Best VAL SAMPLE acc = {best_val_sample_acc*100:.2f}% at epoch {best_choice['epoch']} \"\n",
    "          f\"({best_choice['mode']}, alpha={best_choice['alpha']})\")\n",
    "    print(f\"[{rpm_name}] TEST SAMPLE acc = {test_choice['acc']*100:.2f}% using {test_choice['mode']} alpha={test_choice['alpha']}\")\n",
    "    print(f\"\\n[{rpm_name}] SAMPLE-level report:\\n{report_text}\")\n",
    "    print(f\"[{rpm_name}] Saved all outputs to: {output_dir}\")\n",
    "\n",
    "    return {\n",
    "        \"rpm\": rpm_name,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"best_choice\": best_choice,\n",
    "        \"test_choice\": {k:test_choice[k] for k in [\"mode\",\"alpha\",\"acc\"]},\n",
    "        \"report\": report_text,\n",
    "        \"cm_internal\": cm\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS (EDIT IF NEEDED)\n",
    "# -------------------------\n",
    "# 660 RPM\n",
    "BASE_660 = r\"F:\\20240925\"\n",
    "DIRS_660 = {\n",
    "    \"BF\": os.path.join(BASE_660, \"BF660_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_660, \"GF660_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_660, \"TF660_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_660, \"N660_1\",  \"AE\"),\n",
    "}\n",
    "OUT_660 = r\"E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\"\n",
    "\n",
    "# 720 RPM\n",
    "BASE_720 = r\"F:\\D4B2\\720\"\n",
    "DIRS_720 = {\n",
    "    \"BF\": os.path.join(BASE_720, \"BF720_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_720, \"GF720_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_720, \"TF720_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_720, \"N720_1\",  \"AE\"),\n",
    "}\n",
    "OUT_720 = r\"E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN BOTH\n",
    "# -------------------------\n",
    "res_660 = run_ept_ae_experiment(\"660_RPM\", DIRS_660, OUT_660)\n",
    "res_720 = run_ept_ae_experiment(\"720_RPM\", DIRS_720, OUT_720)\n",
    "\n",
    "print(\"\\n✅ DONE for BOTH 660 & 720 RPM.\")\n",
    "print(\"660 outputs:\", OUT_660)\n",
    "print(\"720 outputs:\", OUT_720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3810994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[660_RPM] Loaded tokens: 115752 | Feature dim: 23\n",
      "[660_RPM] Class counts (tokens): {'BF': 23932, 'GF': 40187, 'TF': 25855, 'N': 25778}\n",
      "[660_RPM] Train tokens: 73505 | Val tokens: 13154 | Test tokens: 29093\n",
      "[660_RPM] Train sample files: 369 | Val sample files: 66 | Test sample files: 146\n",
      "[660_RPM] Epoch 01 | train loss 1.2661 acc 65.62% | val tok acc 73.76% | val SAMPLE acc 98.48% | best 98.48%\n",
      "[660_RPM] Epoch 05 | train loss 0.8998 acc 83.44% | val tok acc 84.77% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 10 | train loss 0.8486 acc 84.74% | val tok acc 86.57% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 15 | train loss 0.8329 acc 86.02% | val tok acc 87.94% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 20 | train loss 0.8258 acc 86.71% | val tok acc 87.72% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 25 | train loss 0.8064 acc 87.28% | val tok acc 88.47% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[660_RPM] Epoch 30 | train loss 0.8034 acc 87.62% | val tok acc 88.98% | val SAMPLE acc 100.00% | best 100.00%\n",
      "    ✓ Saved: 660_RPM_training_curves.png\n",
      "    ✓ Saved: 660_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 660_RPM_EMB_3D_UMAP.png and 660_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[660_RPM] Best VAL SAMPLE acc = 100.00% at epoch 3 (majority, alpha=None)\n",
      "[660_RPM] TEST SAMPLE acc = 100.00% using majority alpha=None\n",
      "\n",
      "[660_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        30\n",
      "          GF     1.0000    1.0000    1.0000        50\n",
      "          TF     1.0000    1.0000    1.0000        33\n",
      "           N     1.0000    1.0000    1.0000        33\n",
      "\n",
      "    accuracy                         1.0000       146\n",
      "   macro avg     1.0000    1.0000    1.0000       146\n",
      "weighted avg     1.0000    1.0000    1.0000       146\n",
      "\n",
      "[660_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\n",
      "\n",
      "[720_RPM] Loaded tokens: 213964 | Feature dim: 29\n",
      "[720_RPM] Class counts (tokens): {'BF': 53287, 'GF': 46885, 'TF': 64450, 'N': 49342}\n",
      "[720_RPM] Train tokens: 136280 | Val tokens: 24201 | Test tokens: 53483\n",
      "[720_RPM] Train sample files: 428 | Val sample files: 76 | Test sample files: 168\n",
      "[720_RPM] Epoch 01 | train loss 1.1451 acc 78.02% | val tok acc 84.55% | val SAMPLE acc 100.00% | best 100.00%\n",
      "[720_RPM] Epoch 05 | train loss 0.7493 acc 76.02% | val tok acc 77.48% | val SAMPLE acc 82.89% | best 100.00%\n",
      "[720_RPM] Epoch 10 | train loss 0.7393 acc 78.47% | val tok acc 79.20% | val SAMPLE acc 92.11% | best 100.00%\n",
      "[720_RPM] Epoch 15 | train loss 0.7365 acc 78.29% | val tok acc 80.18% | val SAMPLE acc 98.68% | best 100.00%\n",
      "[720_RPM] Epoch 20 | train loss 0.7353 acc 78.31% | val tok acc 78.65% | val SAMPLE acc 90.79% | best 100.00%\n",
      "[720_RPM] Epoch 25 | train loss 0.7342 acc 78.65% | val tok acc 80.37% | val SAMPLE acc 98.68% | best 100.00%\n",
      "[720_RPM] Epoch 30 | train loss 0.7347 acc 77.98% | val tok acc 79.35% | val SAMPLE acc 94.74% | best 100.00%\n",
      "    ✓ Saved: 720_RPM_training_curves.png\n",
      "    ✓ Saved: 720_RPM_CM_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_ROC_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_tSNE2D_SAMPLE.png\n",
      "    ✓ Saved: 720_RPM_EMB_3D_UMAP.png and 720_RPM_EMB_3D_UMAP.pdf\n",
      "\n",
      "[720_RPM] Best VAL SAMPLE acc = 100.00% at epoch 1 (majority, alpha=None)\n",
      "[720_RPM] TEST SAMPLE acc = 100.00% using majority alpha=None\n",
      "\n",
      "[720_RPM] SAMPLE-level report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          BF     1.0000    1.0000    1.0000        38\n",
      "          GF     1.0000    1.0000    1.0000        41\n",
      "          TF     1.0000    1.0000    1.0000        47\n",
      "           N     1.0000    1.0000    1.0000        42\n",
      "\n",
      "    accuracy                         1.0000       168\n",
      "   macro avg     1.0000    1.0000    1.0000       168\n",
      "weighted avg     1.0000    1.0000    1.0000       168\n",
      "\n",
      "[720_RPM] Saved all outputs to: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\n",
      "\n",
      "✅ DONE for BOTH 660 & 720 RPM.\n",
      "660: {'rpm': '660_RPM', 'best_choice': {'epoch': 3, 'mode': 'majority', 'alpha': None, 'val_sample_acc': 100.0}, 'test_choice': {'mode': 'majority', 'alpha': None, 'acc': 1.0}}\n",
      "720: {'rpm': '720_RPM', 'best_choice': {'epoch': 1, 'mode': 'majority', 'alpha': None, 'val_sample_acc': 100.0}, 'test_choice': {'mode': 'majority', 'alpha': None, 'acc': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EPT-AE (Energy–Peak Tokenization + Regularized Prototype Classifier)\n",
    "# FINAL ONE-CELL CODE for BOTH 660 & 720 RPM (SAMPLE-LEVEL)\n",
    "#\n",
    "# Improvements vs previous:\n",
    "#  - Best checkpoint based on VAL sample-level accuracy (kept)\n",
    "#  - Auto choose majority vs weighted voting (kept)\n",
    "#  - 720-only upgrades:\n",
    "#       * more events (K_MAD, PEAK_DISTANCE, MAX_TOKENS_PER_FILE)\n",
    "#       * slightly richer features (TOPK_WPT=16 + spectral flatness + spectral kurtosis)\n",
    "#       * token-balanced sampling (WeightedRandomSampler)\n",
    "#       * mild weight decay\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, json\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import pywt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# -------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Splits\n",
    "TEST_SIZE = 0.25\n",
    "VAL_SIZE_FROM_TRAIN = 0.15\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "LAMBDA_COMPACT = 0.2\n",
    "WEIGHT_DECAY_660 = 0.0\n",
    "WEIGHT_DECAY_720 = 1e-4\n",
    "\n",
    "# Aggregation tuning (weighted vote alpha grid)\n",
    "ALPHA_GRID = (3.0, 5.0, 8.0, 10.0, 15.0)\n",
    "\n",
    "# Visualization\n",
    "CLASS_NAMES = [\"BF\", \"GF\", \"TF\", \"N\"]  # desired order\n",
    "ORIGINAL_TO_NEW = {0:0, 1:1, 2:2, 3:3}  # set per experiment\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VISUALIZATION\n",
    "# -------------------------\n",
    "class PublicationVisualizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def remap_labels(labels):\n",
    "        return np.array([ORIGINAL_TO_NEW[int(label)] for label in labels])\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, output_dir, filename):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "        y_pred_r = PublicationVisualizer.remap_labels(y_pred)\n",
    "        cm = confusion_matrix(y_true_r, y_pred_r)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                    cbar=False, annot_kws={\"size\": 22, \"fontweight\": \"bold\"})\n",
    "        plt.xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "        plt.setp(plt.gca().get_xticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.setp(plt.gca().get_yticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(y_true, y_proba, output_dir, filename):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        y_proba_r = np.zeros_like(y_proba)\n",
    "        for old_idx, new_idx in ORIGINAL_TO_NEW.items():\n",
    "            y_proba_r[:, new_idx] = y_proba[:, old_idx]\n",
    "\n",
    "        y_bin = label_binarize(y_true_r, classes=[0, 1, 2, 3])\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "        for i in range(4):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba_r[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "\n",
    "        for i in range(4):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2.5, color=colors[i], linestyle=line_styles[i],\n",
    "                     label=f'{CLASS_NAMES[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=13, frameon=True, framealpha=0.95)\n",
    "        plt.grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.xlim([-0.02, 1.02])\n",
    "        plt.ylim([-0.02, 1.02])\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_2d(features, y_true, output_dir, filename):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        markers = ['o', 's', '^', 'D']\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "        tsne = TSNE(\n",
    "            n_components=2, random_state=42, init='pca',\n",
    "            learning_rate=200, perplexity=perplexity,\n",
    "            n_iter=3000, early_exaggeration=12.0,\n",
    "            metric='euclidean'\n",
    "        )\n",
    "        feat2d = tsne.fit_transform(features)\n",
    "\n",
    "        plt.figure(figsize=(8, 7))\n",
    "        for i, (cname, m, col) in enumerate(zip(CLASS_NAMES, markers, colors)):\n",
    "            sel = (y_true_r == i)\n",
    "            plt.scatter(feat2d[sel, 0], feat2d[sel, 1],\n",
    "                        marker=m, color=col, label=cname, alpha=0.85, s=80,\n",
    "                        edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        plt.legend(title=\"Fault Types\", loc='best',\n",
    "                   prop={'weight': 'bold', 'size': 14}, title_fontsize=15,\n",
    "                   frameon=True, fancybox=True, shadow=True)\n",
    "        plt.xlabel('t-SNE Component 1', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('t-SNE Component 2', fontsize=18, fontweight='bold')\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.grid(alpha=0.2, linestyle='--', linewidth=0.8)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_tsne_3d(features, y_true, output_dir, filename_prefix):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        rcParams['font.family'] = 'Arial'\n",
    "        rcParams['font.size'] = 12\n",
    "        colors_3d = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "        coords_3d = None\n",
    "        method = None\n",
    "        try:\n",
    "            import umap.umap_ as umap\n",
    "            reducer = umap.UMAP(\n",
    "                n_components=3, n_neighbors=15, min_dist=0.3,\n",
    "                metric=\"euclidean\", random_state=42, spread=1.5\n",
    "            )\n",
    "            coords_3d = reducer.fit_transform(features)\n",
    "            method = \"UMAP\"\n",
    "        except Exception:\n",
    "            perplexity = min(30, max(5, (features.shape[0] - 1) // 3))\n",
    "            tsne3 = TSNE(\n",
    "                n_components=3, random_state=42, init=\"pca\",\n",
    "                learning_rate=200, perplexity=perplexity,\n",
    "                n_iter=3000, early_exaggeration=12.0\n",
    "            )\n",
    "            coords_3d = tsne3.fit_transform(features)\n",
    "            method = \"t-SNE\"\n",
    "\n",
    "        fig = plt.figure(figsize=(10, 8), facecolor='white')\n",
    "        ax = fig.add_subplot(111, projection='3d', facecolor='white')\n",
    "\n",
    "        for i, cname in enumerate(CLASS_NAMES):\n",
    "            sel = (y_true_r == i)\n",
    "            ax.scatter(coords_3d[sel, 0], coords_3d[sel, 1], coords_3d[sel, 2],\n",
    "                       c=colors_3d[i], marker='o', label=cname,\n",
    "                       alpha=0.9, s=60, edgecolors='black', linewidth=0.8)\n",
    "\n",
    "        ax.set_xlabel(f'{method} Component 1', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_ylabel(f'{method} Component 2', fontsize=16, fontweight='bold', labelpad=15)\n",
    "        ax.set_zlabel(f'{method} Component 3', fontsize=16, fontweight='bold', labelpad=15)\n",
    "\n",
    "        ax.legend(loc='upper right', fontsize=12, frameon=True,\n",
    "                  prop={'weight': 'bold'}, fancybox=True, shadow=True)\n",
    "        ax.grid(True, alpha=0.25, linestyle='--', linewidth=0.8, color='gray')\n",
    "\n",
    "        for pane in [ax.xaxis.pane, ax.yaxis.pane, ax.zaxis.pane]:\n",
    "            pane.fill = True\n",
    "            pane.set_facecolor('white')\n",
    "            pane.set_alpha(0.1)\n",
    "            pane.set_edgecolor('lightgray')\n",
    "\n",
    "        ax.view_init(elev=15, azim=45)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        png_filename = f\"{filename_prefix}_3D_{method}.png\"\n",
    "        pdf_filename = f\"{filename_prefix}_3D_{method}.pdf\"\n",
    "        plt.savefig(os.path.join(output_dir, png_filename), dpi=600, bbox_inches='tight', facecolor='white')\n",
    "        plt.savefig(os.path.join(output_dir, pdf_filename), bbox_inches='tight', facecolor='white')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {png_filename} and {pdf_filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_curves(history, output_dir, filename):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "        axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2.5)\n",
    "        axes[0].plot(epochs, history['val_loss'], label='Validation', linewidth=2.5)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[0].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[0].set_title('Token-level Loss', fontsize=14, fontweight='bold')\n",
    "\n",
    "        axes[1].plot(epochs, history['train_acc'], label='Train', linewidth=2.5)\n",
    "        axes[1].plot(epochs, history['val_acc'], label='Validation', linewidth=2.5)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[1].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[1].set_title('Token-level Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "        axes[2].plot(epochs, history['val_sample_acc'], label='Val SAMPLE', linewidth=2.5)\n",
    "        axes[2].set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        axes[2].set_ylabel('Accuracy (%)', fontsize=14, fontweight='bold')\n",
    "        axes[2].legend(fontsize=12, prop={'weight': 'bold'}, frameon=True)\n",
    "        axes[2].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[2].set_title('VAL SAMPLE Accuracy', fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"    ✓ Saved: {filename}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SIGNAL + FEATURES\n",
    "# -------------------------\n",
    "def robust_mad(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-12\n",
    "    return med, mad\n",
    "\n",
    "def find_1d_signal_in_mat(mat_dict):\n",
    "    candidates = []\n",
    "    for k, v in mat_dict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "            arr = np.array(v).squeeze()\n",
    "            if arr.ndim == 1 and arr.size > 1000:\n",
    "                candidates.append((k, arr.size, arr))\n",
    "    if not candidates:\n",
    "        for k, v in mat_dict.items():\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "                arr = np.array(v).squeeze()\n",
    "                if arr.size > 1000:\n",
    "                    return k, arr.reshape(-1).astype(np.float32)\n",
    "        raise ValueError(\"No suitable numeric signal array found in .mat file.\")\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0], candidates[0][2].astype(np.float32)\n",
    "\n",
    "def short_time_energy(x, win):\n",
    "    x2 = x.astype(np.float64) ** 2\n",
    "    kernel = np.ones(win, dtype=np.float64)\n",
    "    E = np.convolve(x2, kernel, mode=\"same\")\n",
    "    return E.astype(np.float32)\n",
    "\n",
    "def energy_peak_tokenize(x, energy_win, k_mad, peak_distance, seg_len, max_tokens):\n",
    "    if x.size < seg_len + 10:\n",
    "        return []\n",
    "    E = short_time_energy(x, energy_win)\n",
    "    med, mad = robust_mad(E)\n",
    "    thr = med + k_mad * mad\n",
    "\n",
    "    peaks, props = find_peaks(E, height=thr, distance=peak_distance)\n",
    "    if peaks.size == 0:\n",
    "        mid = x.size // 2\n",
    "        half = seg_len // 2\n",
    "        seg = x[max(0, mid-half): min(x.size, mid+half)]\n",
    "        return [seg.astype(np.float32)] if seg.size == seg_len else []\n",
    "\n",
    "    heights = props.get(\"peak_heights\", E[peaks])\n",
    "    order = np.argsort(heights)[::-1]\n",
    "    peaks = peaks[order][:max_tokens]\n",
    "\n",
    "    half = seg_len // 2\n",
    "    tokens = []\n",
    "    for p in peaks:\n",
    "        s = p - half\n",
    "        e = p + half\n",
    "        if s < 0 or e > x.size:\n",
    "            continue\n",
    "        seg = x[s:e].astype(np.float32)\n",
    "        if seg.size == seg_len:\n",
    "            tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "def time_features(seg):\n",
    "    x = seg.astype(np.float64)\n",
    "    x0 = x - np.mean(x)\n",
    "    rms = np.sqrt(np.mean(x0**2) + 1e-12)\n",
    "    peak = np.max(np.abs(x0)) + 1e-12\n",
    "    ptp = np.ptp(x0)\n",
    "    crest = peak / (rms + 1e-12)\n",
    "    kurt = kurtosis(x0, fisher=False, bias=False) if x0.size > 10 else 0.0\n",
    "    sk = skew(x0, bias=False) if x0.size > 10 else 0.0\n",
    "    return np.array([rms, peak, ptp, crest, kurt, sk], dtype=np.float32)\n",
    "\n",
    "def spectral_features(seg, fft_n, eps=1e-12):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    n = min(len(x), fft_n)\n",
    "    w = np.hanning(n)\n",
    "    xw = x[:n] * w\n",
    "    X = np.fft.rfft(xw, n=n)\n",
    "    mag = np.abs(X) + eps\n",
    "    psd = mag**2 + eps\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)  # normalized\n",
    "\n",
    "    psd_sum = np.sum(psd) + eps\n",
    "    p = psd / psd_sum\n",
    "\n",
    "    centroid = np.sum(freqs * psd) / psd_sum\n",
    "    bandwidth = np.sqrt(np.sum(((freqs - centroid) ** 2) * psd) / psd_sum)\n",
    "    entropy = -np.sum(p * np.log(p + eps))\n",
    "    dom_idx = int(np.argmax(psd))\n",
    "    dom_freq = freqs[dom_idx]\n",
    "    rolloff_85 = freqs[np.searchsorted(np.cumsum(psd) / psd_sum, 0.85)]\n",
    "\n",
    "    # Added (helps GF vs N at 720)\n",
    "    flatness = np.exp(np.mean(np.log(psd))) / (np.mean(psd) + eps)\n",
    "    psd_kurt = kurtosis(psd, fisher=False, bias=False) if psd.size > 10 else 0.0\n",
    "\n",
    "    return np.array([centroid, bandwidth, entropy, dom_freq, rolloff_85, flatness, psd_kurt], dtype=np.float32)\n",
    "\n",
    "def wpt_topk_energy(seg, wavelet, level, topk):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet=wavelet, mode=\"symmetric\", maxlevel=level)\n",
    "    nodes = wp.get_level(level, order=\"freq\")\n",
    "    energies = np.array([np.sum(n.data**2) for n in nodes], dtype=np.float64)\n",
    "    energies = energies / (np.sum(energies) + 1e-12)\n",
    "    top = np.sort(energies)[::-1][:topk]\n",
    "    return top.astype(np.float32)\n",
    "\n",
    "def extract_features_from_token(seg, fft_n, wavelet, level, topk_wpt):\n",
    "    tf = time_features(seg)\n",
    "    sf = spectral_features(seg, fft_n=fft_n)\n",
    "    wf = wpt_topk_energy(seg, wavelet=wavelet, level=level, topk=topk_wpt)\n",
    "    return np.concatenate([tf, sf, wf], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TORCH DATA + MODEL\n",
    "# -------------------------\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, X, y, sid):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.sid = torch.tensor(sid, dtype=torch.long)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.sid[idx]\n",
    "\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "def compute_prototypes(model, loader, num_classes, emb_dim):\n",
    "    model.eval()\n",
    "    sums = torch.zeros((num_classes, emb_dim), device=DEVICE)\n",
    "    counts = torch.zeros((num_classes,), device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "            zb = model(xb)\n",
    "            for c in range(num_classes):\n",
    "                m = (yb == c)\n",
    "                if m.any():\n",
    "                    sums[c] += zb[m].sum(dim=0)\n",
    "                    counts[c] += m.sum()\n",
    "    protos = sums / (counts.unsqueeze(1) + 1e-12)\n",
    "    return F.normalize(protos, dim=1)\n",
    "\n",
    "def logits_from_prototypes(z, protos):\n",
    "    return z @ protos.t()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAMPLE-LEVEL AGGREGATION\n",
    "# -------------------------\n",
    "def sample_level_eval(model, loader, sids_set, sample_true_dict, protos, num_classes, alphas):\n",
    "    model.eval()\n",
    "\n",
    "    tok_sid = []\n",
    "    tok_pred_major = []\n",
    "    tok_prob_by_alpha = {a: [] for a in alphas}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, _, sidb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            tok_pred_major.extend(pred.tolist())\n",
    "            tok_sid.extend(sidb.numpy().tolist())\n",
    "\n",
    "            for a in alphas:\n",
    "                probs = torch.softmax(a * logits, dim=1).cpu().numpy()\n",
    "                tok_prob_by_alpha[a].append(probs)\n",
    "\n",
    "    tok_sid = np.array(tok_sid, dtype=np.int64)\n",
    "    tok_pred_major = np.array(tok_pred_major, dtype=np.int64)\n",
    "\n",
    "    # Majority aggregation\n",
    "    votes = {sid: [] for sid in sids_set}\n",
    "    for sid, p in zip(tok_sid, tok_pred_major):\n",
    "        if sid in votes:\n",
    "            votes[sid].append(p)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_major = []\n",
    "    for sid in sorted(votes.keys()):\n",
    "        y_true.append(sample_true_dict[sid])\n",
    "        vals = votes[sid]\n",
    "        if len(vals) == 0:\n",
    "            y_pred_major.append(0)\n",
    "        else:\n",
    "            u, c = np.unique(vals, return_counts=True)\n",
    "            y_pred_major.append(int(u[np.argmax(c)]))\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.int64)\n",
    "    y_pred_major = np.array(y_pred_major, dtype=np.int64)\n",
    "    acc_major = float((y_true == y_pred_major).mean())\n",
    "\n",
    "    proba_major = np.zeros((len(y_pred_major), num_classes), dtype=np.float32)\n",
    "    for i,p in enumerate(y_pred_major):\n",
    "        proba_major[i,p] = 1.0\n",
    "\n",
    "    best = {\"mode\":\"majority\", \"alpha\":None, \"acc\":acc_major,\n",
    "            \"y_true\":y_true, \"y_pred\":y_pred_major, \"y_proba\":proba_major}\n",
    "\n",
    "    # Weighted aggregation (alpha sweep)\n",
    "    for a in alphas:\n",
    "        probs_all = np.concatenate(tok_prob_by_alpha[a], axis=0)\n",
    "        scores = {sid: np.zeros((num_classes,), dtype=np.float64) for sid in sids_set}\n",
    "        for sid, pv in zip(tok_sid, probs_all):\n",
    "            if sid in scores:\n",
    "                scores[sid] += pv\n",
    "\n",
    "        y_pred = []\n",
    "        y_proba = []\n",
    "        for sid in sorted(scores.keys()):\n",
    "            sc = scores[sid]\n",
    "            y_pred.append(int(np.argmax(sc)))\n",
    "            y_proba.append((sc / (np.sum(sc) + 1e-12)).astype(np.float32))\n",
    "\n",
    "        y_pred = np.array(y_pred, dtype=np.int64)\n",
    "        acc = float((y_true == y_pred).mean())\n",
    "        if acc > best[\"acc\"]:\n",
    "            best = {\"mode\":\"weighted\", \"alpha\":float(a), \"acc\":acc,\n",
    "                    \"y_true\":y_true, \"y_pred\":y_pred, \"y_proba\":np.stack(y_proba, axis=0)}\n",
    "\n",
    "    return best\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# EXPERIMENT RUNNER\n",
    "# -------------------------\n",
    "def run_ept_ae_experiment(rpm_name, class_dirs, output_dir, rpm_cfg):\n",
    "    global ORIGINAL_TO_NEW\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    class_order = list(class_dirs.keys())\n",
    "    label_map = {cls:i for i, cls in enumerate(class_order)}\n",
    "    ORIGINAL_TO_NEW = {label_map[c]: CLASS_NAMES.index(c) for c in CLASS_NAMES}\n",
    "\n",
    "    # RPM-specific hyperparams\n",
    "    ENERGY_WIN = rpm_cfg[\"ENERGY_WIN\"]\n",
    "    K_MAD = rpm_cfg[\"K_MAD\"]\n",
    "    PEAK_DISTANCE = rpm_cfg[\"PEAK_DISTANCE\"]\n",
    "    SEG_LEN = rpm_cfg[\"SEG_LEN\"]\n",
    "    MAX_TOKENS_PER_FILE = rpm_cfg[\"MAX_TOKENS_PER_FILE\"]\n",
    "    FFT_N = rpm_cfg[\"FFT_N\"]\n",
    "    TOPK_WPT = rpm_cfg[\"TOPK_WPT\"]\n",
    "    WPT_WAVELET = rpm_cfg[\"WPT_WAVELET\"]\n",
    "    WPT_LEVEL = rpm_cfg[\"WPT_LEVEL\"]\n",
    "    WEIGHT_DECAY = rpm_cfg[\"WEIGHT_DECAY\"]\n",
    "    USE_BALANCED_SAMPLER = rpm_cfg[\"USE_BALANCED_SAMPLER\"]\n",
    "\n",
    "    # ---- Load -> Tokenize -> Features\n",
    "    all_X, all_y, all_sid = [], [], []\n",
    "    sample_meta = []  # (sid, class_name, filepath)\n",
    "    sid_counter = 0\n",
    "\n",
    "    for cls_name, folder in class_dirs.items():\n",
    "        mats = sorted(glob.glob(os.path.join(folder, \"*.mat\")))\n",
    "        if len(mats) == 0:\n",
    "            print(f\"[WARN] No .mat files found in: {folder}\")\n",
    "\n",
    "        for fp in mats:\n",
    "            try:\n",
    "                md = loadmat(fp)\n",
    "                _, sig = find_1d_signal_in_mat(md)\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP] {fp} ({e})\")\n",
    "                continue\n",
    "\n",
    "            tokens = energy_peak_tokenize(\n",
    "                sig, energy_win=ENERGY_WIN, k_mad=K_MAD,\n",
    "                peak_distance=PEAK_DISTANCE, seg_len=SEG_LEN,\n",
    "                max_tokens=MAX_TOKENS_PER_FILE\n",
    "            )\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "\n",
    "            sid = sid_counter\n",
    "            sid_counter += 1\n",
    "            sample_meta.append((sid, cls_name, fp))\n",
    "\n",
    "            for seg in tokens:\n",
    "                all_X.append(extract_features_from_token(\n",
    "                    seg, fft_n=FFT_N, wavelet=WPT_WAVELET, level=WPT_LEVEL, topk_wpt=TOPK_WPT\n",
    "                ))\n",
    "                all_y.append(label_map[cls_name])\n",
    "                all_sid.append(sid)\n",
    "\n",
    "    all_X = np.stack(all_X, axis=0)\n",
    "    all_y = np.array(all_y, dtype=np.int64)\n",
    "    all_sid = np.array(all_sid, dtype=np.int64)\n",
    "\n",
    "    inv_label = {v:k for k,v in label_map.items()}\n",
    "    token_counts = {inv_label[i]: int(np.sum(all_y == i)) for i in sorted(inv_label)}\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Loaded tokens: {all_X.shape[0]} | Feature dim: {all_X.shape[1]}\")\n",
    "    print(f\"[{rpm_name}] Class counts (tokens): {token_counts}\")\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_dataset_summary.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rpm\": rpm_name,\n",
    "            \"num_tokens\": int(all_X.shape[0]),\n",
    "            \"feature_dim\": int(all_X.shape[1]),\n",
    "            \"token_counts\": token_counts,\n",
    "            \"num_sample_files\": int(len(sample_meta)),\n",
    "            \"rpm_cfg\": rpm_cfg,\n",
    "            \"ORIGINAL_TO_NEW\": ORIGINAL_TO_NEW\n",
    "        }, f, indent=2)\n",
    "\n",
    "    # ---- Split by SAMPLE FILE\n",
    "    sample_ids = np.array([m[0] for m in sample_meta], dtype=np.int64)\n",
    "    sample_labels = np.array([label_map[m[1]] for m in sample_meta], dtype=np.int64)\n",
    "\n",
    "    train_sids, test_sids = train_test_split(\n",
    "        sample_ids, test_size=TEST_SIZE, random_state=SEED, stratify=sample_labels\n",
    "    )\n",
    "    train_labels = sample_labels[np.isin(sample_ids, train_sids)]\n",
    "    train_sids, val_sids = train_test_split(\n",
    "        train_sids, test_size=VAL_SIZE_FROM_TRAIN, random_state=SEED, stratify=train_labels\n",
    "    )\n",
    "\n",
    "    train_mask = np.isin(all_sid, train_sids)\n",
    "    val_mask   = np.isin(all_sid, val_sids)\n",
    "    test_mask  = np.isin(all_sid, test_sids)\n",
    "\n",
    "    X_train, y_train, sid_train = all_X[train_mask], all_y[train_mask], all_sid[train_mask]\n",
    "    X_val,   y_val,   sid_val   = all_X[val_mask],   all_y[val_mask],   all_sid[val_mask]\n",
    "    X_test,  y_test,  sid_test  = all_X[test_mask],  all_y[test_mask],  all_sid[test_mask]\n",
    "\n",
    "    print(f\"[{rpm_name}] Train tokens: {X_train.shape[0]} | Val tokens: {X_val.shape[0]} | Test tokens: {X_test.shape[0]}\")\n",
    "    print(f\"[{rpm_name}] Train sample files: {len(train_sids)} | Val sample files: {len(val_sids)} | Test sample files: {len(test_sids)}\")\n",
    "\n",
    "    # ---- Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "    X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "    X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_scaler_stats.npz\"),\n",
    "             mean=scaler.mean_, scale=scaler.scale_)\n",
    "\n",
    "    # ---- Datasets\n",
    "    train_ds = TokenDataset(X_train, y_train, sid_train)\n",
    "    val_ds   = TokenDataset(X_val,   y_val,   sid_val)\n",
    "    test_ds  = TokenDataset(X_test,  y_test,  sid_test)\n",
    "\n",
    "    # ---- Loaders (balanced sampler for 720)\n",
    "    if USE_BALANCED_SAMPLER:\n",
    "        class_counts = np.bincount(y_train, minlength=len(class_order)).astype(np.float64)\n",
    "        class_weights = 1.0 / (class_counts + 1e-12)\n",
    "        sample_weights = class_weights[y_train]\n",
    "        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "    val_loader  = DataLoader(val_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    # ---- Model\n",
    "    num_classes = len(class_order)\n",
    "    emb_dim = 64\n",
    "    model = EmbNet(in_dim=X_train.shape[1], emb_dim=emb_dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # sample_true dicts\n",
    "    meta_dict = {sid: cls for sid, cls, _ in sample_meta}\n",
    "    val_true  = {sid: label_map[meta_dict[sid]] for sid in val_sids}\n",
    "    test_true = {sid: label_map[meta_dict[sid]] for sid in test_sids}\n",
    "\n",
    "    # ---- Training with best checkpoint by VAL sample-level\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[], \"val_sample_acc\":[]}\n",
    "    best_state = None\n",
    "    best_val_sample_acc = -1.0\n",
    "    best_choice = None\n",
    "\n",
    "    def token_eval_loss_acc(loader, protos):\n",
    "        model.eval()\n",
    "        total_loss, total_correct, n = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in loader:\n",
    "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "                z = model(xb)\n",
    "                logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "                ce = F.cross_entropy(logits, yb)\n",
    "                p_y = protos[yb]\n",
    "                comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "                loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "                bs = xb.size(0)\n",
    "                total_loss += loss.item() * bs\n",
    "                total_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "                n += bs\n",
    "        return total_loss/max(n,1), 100.0*(total_correct/max(n,1))\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                    num_classes=num_classes, emb_dim=emb_dim)\n",
    "\n",
    "        model.train()\n",
    "        run_loss, run_correct, seen = 0.0, 0, 0\n",
    "\n",
    "        for xb, yb, _ in train_loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            ce = F.cross_entropy(logits, yb)\n",
    "            p_y = protos[yb]\n",
    "            comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "            loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            run_loss += loss.item() * bs\n",
    "            run_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "            seen += bs\n",
    "\n",
    "        train_loss = run_loss/max(seen,1)\n",
    "        train_acc = 100.0*(run_correct/max(seen,1))\n",
    "        val_loss, val_acc = token_eval_loss_acc(val_loader, protos)\n",
    "\n",
    "        val_choice = sample_level_eval(model, val_loader, set(val_sids), val_true, protos,\n",
    "                                       num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "        val_sample_acc = 100.0 * val_choice[\"acc\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_sample_acc\"].append(val_sample_acc)\n",
    "\n",
    "        if val_choice[\"acc\"] > best_val_sample_acc:\n",
    "            best_val_sample_acc = val_choice[\"acc\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            best_choice = {\"epoch\": epoch, \"mode\": val_choice[\"mode\"], \"alpha\": val_choice[\"alpha\"],\n",
    "                           \"val_sample_acc\": float(val_sample_acc)}\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0 or epoch == EPOCHS:\n",
    "            print(f\"[{rpm_name}] Epoch {epoch:02d} | train loss {train_loss:.4f} acc {train_acc:.2f}% | \"\n",
    "                  f\"val tok acc {val_acc:.2f}% | val SAMPLE acc {val_sample_acc:.2f}% | best {best_val_sample_acc*100:.2f}%\")\n",
    "\n",
    "    # restore best\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                num_classes=num_classes, emb_dim=emb_dim)\n",
    "\n",
    "    PublicationVisualizer.plot_training_curves(history, output_dir, f\"{rpm_name}_training_curves.png\")\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_history.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_best_selection.json\"), \"w\") as f:\n",
    "        json.dump(best_choice, f, indent=2)\n",
    "\n",
    "    # TEST sample-level\n",
    "    test_choice = sample_level_eval(model, test_loader, set(test_sids), test_true, protos,\n",
    "                                    num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "    y_true_s = test_choice[\"y_true\"]\n",
    "    y_pred_s = test_choice[\"y_pred\"]\n",
    "    y_proba_s = test_choice[\"y_proba\"]\n",
    "\n",
    "    report_text = classification_report(y_true_s, y_pred_s, target_names=class_order, digits=4)\n",
    "    cm = confusion_matrix(y_true_s, y_pred_s)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_report.txt\"), \"w\") as f:\n",
    "        f.write(f\"RPM: {rpm_name}\\n\")\n",
    "        f.write(\"Internal class order: \" + str(class_order) + \"\\n\")\n",
    "        f.write(\"Plot class order: \" + str(CLASS_NAMES) + \"\\n\")\n",
    "        f.write(\"ORIGINAL_TO_NEW: \" + str(ORIGINAL_TO_NEW) + \"\\n\")\n",
    "        f.write(\"RPM config: \" + json.dumps(rpm_cfg, indent=2) + \"\\n\")\n",
    "        f.write(\"Best checkpoint (VAL SAMPLE): \" + str(best_choice) + \"\\n\")\n",
    "        f.write(\"Test aggregation: \" + str({k:test_choice[k] for k in ['mode','alpha','acc']}) + \"\\n\\n\")\n",
    "        f.write(report_text + \"\\n\\n\")\n",
    "        f.write(\"Confusion matrix (internal order):\\n\")\n",
    "        f.write(np.array2string(cm) + \"\\n\")\n",
    "\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_level_outputs.npz\"),\n",
    "             y_true=y_true_s, y_pred=y_pred_s, y_proba=y_proba_s)\n",
    "\n",
    "    PublicationVisualizer.plot_confusion_matrix(y_true_s, y_pred_s, output_dir, f\"{rpm_name}_CM_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_roc_curves(y_true_s, y_proba_s, output_dir, f\"{rpm_name}_ROC_SAMPLE.png\")\n",
    "\n",
    "    # Sample embeddings for t-SNE (mean token embedding per file)\n",
    "    model.eval()\n",
    "    emb_sum = {sid: np.zeros((emb_dim,), dtype=np.float64) for sid in test_sids}\n",
    "    emb_cnt = {sid: 0 for sid in test_sids}\n",
    "    with torch.no_grad():\n",
    "        for xb, _, sidb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb).cpu().numpy()\n",
    "            sids = sidb.numpy()\n",
    "            for zi, si in zip(z, sids):\n",
    "                si = int(si)\n",
    "                if si in emb_sum:\n",
    "                    emb_sum[si] += zi\n",
    "                    emb_cnt[si] += 1\n",
    "\n",
    "    sample_ids_sorted = sorted(list(test_sids))\n",
    "    sample_emb = []\n",
    "    sample_y_for_tsne = []\n",
    "    for sid in sample_ids_sorted:\n",
    "        if emb_cnt[sid] > 0:\n",
    "            sample_emb.append((emb_sum[sid] / emb_cnt[sid]).astype(np.float32))\n",
    "        else:\n",
    "            sample_emb.append(np.zeros((emb_dim,), dtype=np.float32))\n",
    "        sample_y_for_tsne.append(test_true[sid])\n",
    "\n",
    "    sample_emb = np.stack(sample_emb, axis=0)\n",
    "    sample_y_for_tsne = np.array(sample_y_for_tsne, dtype=np.int64)\n",
    "\n",
    "    PublicationVisualizer.plot_tsne_2d(sample_emb, sample_y_for_tsne, output_dir, f\"{rpm_name}_tSNE2D_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_tsne_3d(sample_emb, sample_y_for_tsne, output_dir, f\"{rpm_name}_EMB\")\n",
    "\n",
    "    np.savez(os.path.join(output_dir, f\"{rpm_name}_SAMPLE_embeddings.npz\"),\n",
    "             sample_ids=np.array(sample_ids_sorted, dtype=np.int64),\n",
    "             emb=sample_emb, y_true=sample_y_for_tsne)\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Best VAL SAMPLE acc = {best_val_sample_acc*100:.2f}% at epoch {best_choice['epoch']} \"\n",
    "          f\"({best_choice['mode']}, alpha={best_choice['alpha']})\")\n",
    "    print(f\"[{rpm_name}] TEST SAMPLE acc = {test_choice['acc']*100:.2f}% using {test_choice['mode']} alpha={test_choice['alpha']}\")\n",
    "    print(f\"\\n[{rpm_name}] SAMPLE-level report:\\n{report_text}\")\n",
    "    print(f\"[{rpm_name}] Saved all outputs to: {output_dir}\")\n",
    "\n",
    "    return {\"rpm\": rpm_name, \"best_choice\": best_choice, \"test_choice\": {k:test_choice[k] for k in [\"mode\",\"alpha\",\"acc\"]}}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS\n",
    "# -------------------------\n",
    "BASE_660 = r\"F:\\20240925\"\n",
    "DIRS_660 = {\n",
    "    \"BF\": os.path.join(BASE_660, \"BF660_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_660, \"GF660_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_660, \"TF660_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_660, \"N660_1\",  \"AE\"),\n",
    "}\n",
    "OUT_660 = r\"E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\"\n",
    "\n",
    "BASE_720 = r\"F:\\D4B2\\720\"\n",
    "DIRS_720 = {\n",
    "    \"BF\": os.path.join(BASE_720, \"BF720_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_720, \"GF720_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_720, \"TF720_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_720, \"N720_1\",  \"AE\"),\n",
    "}\n",
    "OUT_720 = r\"E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RPM-SPECIFIC CONFIGS\n",
    "# -------------------------\n",
    "CFG_660 = dict(\n",
    "    ENERGY_WIN=256,\n",
    "    K_MAD=6.0,\n",
    "    PEAK_DISTANCE=800,\n",
    "    SEG_LEN=4096,\n",
    "    MAX_TOKENS_PER_FILE=200,\n",
    "    FFT_N=2048,\n",
    "    WPT_WAVELET=\"db4\",\n",
    "    WPT_LEVEL=5,\n",
    "    TOPK_WPT=10,             # keep small (already perfect)\n",
    "    WEIGHT_DECAY=WEIGHT_DECAY_660,\n",
    "    USE_BALANCED_SAMPLER=False\n",
    ")\n",
    "\n",
    "CFG_720 = dict(\n",
    "    ENERGY_WIN=256,\n",
    "    K_MAD=5.0,               # more tokens\n",
    "    PEAK_DISTANCE=600,       # more tokens\n",
    "    SEG_LEN=4096,\n",
    "    MAX_TOKENS_PER_FILE=350, # more evidence per file\n",
    "    FFT_N=2048,\n",
    "    WPT_WAVELET=\"db4\",\n",
    "    WPT_LEVEL=5,\n",
    "    TOPK_WPT=16,             # richer WPT energy signature\n",
    "    WEIGHT_DECAY=WEIGHT_DECAY_720,\n",
    "    USE_BALANCED_SAMPLER=True\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN BOTH\n",
    "# -------------------------\n",
    "res_660 = run_ept_ae_experiment(\"660_RPM\", DIRS_660, OUT_660, CFG_660)\n",
    "res_720 = run_ept_ae_experiment(\"720_RPM\", DIRS_720, OUT_720, CFG_720)\n",
    "\n",
    "print(\"\\n✅ DONE for BOTH 660 & 720 RPM.\")\n",
    "print(\"660:\", res_660)\n",
    "print(\"720:\", res_720)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0ba1521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= 660 RPM: CV5 =================\n",
      "[660_RPM] Fold 1/5 -> TEST sample-acc: 100.00%\n",
      "[660_RPM] Fold 2/5 -> TEST sample-acc: 100.00%\n",
      "[660_RPM] Fold 3/5 -> TEST sample-acc: 100.00%\n",
      "[660_RPM] Fold 4/5 -> TEST sample-acc: 100.00%\n",
      "[660_RPM] Fold 5/5 -> TEST sample-acc: 100.00%\n",
      "\n",
      "[660_RPM] CV5 mean acc = 100.00% ± 0.00%\n",
      "[660_RPM] Saved CV5 to: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\\CV5\n",
      "\n",
      "================= 660 RPM: LowData =================\n",
      "[660_RPM] LowData 10% | Fold 1 -> 100.00%\n",
      "[660_RPM] LowData 10% | Fold 2 -> 98.28%\n",
      "[660_RPM] LowData 10% | Fold 3 -> 100.00%\n",
      "[660_RPM] LowData 10% | Fold 4 -> 100.00%\n",
      "[660_RPM] LowData 10% | Fold 5 -> 100.00%\n",
      "[660_RPM] LowData 20% | Fold 1 -> 98.29%\n",
      "[660_RPM] LowData 20% | Fold 2 -> 100.00%\n",
      "[660_RPM] LowData 20% | Fold 3 -> 100.00%\n",
      "[660_RPM] LowData 20% | Fold 4 -> 100.00%\n",
      "[660_RPM] LowData 20% | Fold 5 -> 100.00%\n",
      "[660_RPM] LowData 40% | Fold 1 -> 100.00%\n",
      "[660_RPM] LowData 40% | Fold 2 -> 100.00%\n",
      "[660_RPM] LowData 40% | Fold 3 -> 100.00%\n",
      "[660_RPM] LowData 40% | Fold 4 -> 100.00%\n",
      "[660_RPM] LowData 40% | Fold 5 -> 100.00%\n",
      "[660_RPM] LowData 60% | Fold 1 -> 100.00%\n",
      "[660_RPM] LowData 60% | Fold 2 -> 100.00%\n",
      "[660_RPM] LowData 60% | Fold 3 -> 100.00%\n",
      "[660_RPM] LowData 60% | Fold 4 -> 100.00%\n",
      "[660_RPM] LowData 60% | Fold 5 -> 100.00%\n",
      "[660_RPM] LowData 100% | Fold 1 -> 100.00%\n",
      "[660_RPM] LowData 100% | Fold 2 -> 100.00%\n",
      "[660_RPM] LowData 100% | Fold 3 -> 100.00%\n",
      "[660_RPM] LowData 100% | Fold 4 -> 100.00%\n",
      "[660_RPM] LowData 100% | Fold 5 -> 100.00%\n",
      "\n",
      "[660_RPM] Saved LowData results to: E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\\LowData\n",
      "\n",
      "================= 720 RPM: CV5 =================\n",
      "[720_RPM] Fold 1/5 -> TEST sample-acc: 97.78%\n",
      "[720_RPM] Fold 2/5 -> TEST sample-acc: 95.56%\n",
      "[720_RPM] Fold 3/5 -> TEST sample-acc: 99.25%\n",
      "[720_RPM] Fold 4/5 -> TEST sample-acc: 97.76%\n",
      "[720_RPM] Fold 5/5 -> TEST sample-acc: 100.00%\n",
      "\n",
      "[720_RPM] CV5 mean acc = 98.07% ± 1.53%\n",
      "[720_RPM] Saved CV5 to: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\\CV5\n",
      "\n",
      "================= 720 RPM: LowData =================\n",
      "[720_RPM] LowData 10% | Fold 1 -> 97.78%\n",
      "[720_RPM] LowData 10% | Fold 2 -> 100.00%\n",
      "[720_RPM] LowData 10% | Fold 3 -> 99.25%\n",
      "[720_RPM] LowData 10% | Fold 4 -> 100.00%\n",
      "[720_RPM] LowData 10% | Fold 5 -> 100.00%\n",
      "[720_RPM] LowData 20% | Fold 1 -> 99.26%\n",
      "[720_RPM] LowData 20% | Fold 2 -> 99.26%\n",
      "[720_RPM] LowData 20% | Fold 3 -> 99.25%\n",
      "[720_RPM] LowData 20% | Fold 4 -> 100.00%\n",
      "[720_RPM] LowData 20% | Fold 5 -> 100.00%\n",
      "[720_RPM] LowData 40% | Fold 1 -> 97.04%\n",
      "[720_RPM] LowData 40% | Fold 2 -> 96.30%\n",
      "[720_RPM] LowData 40% | Fold 3 -> 100.00%\n",
      "[720_RPM] LowData 40% | Fold 4 -> 98.51%\n",
      "[720_RPM] LowData 40% | Fold 5 -> 95.52%\n",
      "[720_RPM] LowData 60% | Fold 1 -> 99.26%\n",
      "[720_RPM] LowData 60% | Fold 2 -> 94.07%\n",
      "[720_RPM] LowData 60% | Fold 3 -> 99.25%\n",
      "[720_RPM] LowData 60% | Fold 4 -> 97.76%\n",
      "[720_RPM] LowData 60% | Fold 5 -> 100.00%\n",
      "[720_RPM] LowData 100% | Fold 1 -> 97.04%\n",
      "[720_RPM] LowData 100% | Fold 2 -> 98.52%\n",
      "[720_RPM] LowData 100% | Fold 3 -> 97.01%\n",
      "[720_RPM] LowData 100% | Fold 4 -> 100.00%\n",
      "[720_RPM] LowData 100% | Fold 5 -> 100.00%\n",
      "\n",
      "[720_RPM] Saved LowData results to: E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\\LowData\n",
      "\n",
      "✅ ALL DONE. Results saved under:\n",
      " - E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\\CV5 and E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\\LowData\n",
      " - E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\\CV5 and E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\\LowData\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EPT-AE FINAL: 5-FOLD FILE-LEVEL CV + LOW-DATA CURVES (660 & 720)\n",
    "# One-cell end-to-end, saves everything to your result folders.\n",
    "# ============================================================\n",
    "\n",
    "import os, glob, random, json, math\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "import pywt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# -------------------------\n",
    "# GLOBAL CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "LAMBDA_COMPACT = 0.2\n",
    "ALPHA_GRID = (3.0, 5.0, 8.0, 10.0, 15.0)\n",
    "\n",
    "# CV\n",
    "N_FOLDS = 5\n",
    "VAL_SIZE_FROM_TRAIN = 0.15\n",
    "MIN_EPOCH_SAVE = 5  # to avoid \"epoch 1 best\" suspicion\n",
    "\n",
    "# Visualization\n",
    "CLASS_NAMES = [\"BF\", \"GF\", \"TF\", \"N\"]\n",
    "ORIGINAL_TO_NEW = {0:0, 1:1, 2:2, 3:3}\n",
    "\n",
    "# Low-data (fractions of TRAIN-FOLD files per class)\n",
    "LOWDATA_FRACS = [0.10, 0.20, 0.40, 0.60, 1.00]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# REPRODUCIBILITY\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# VISUALIZATION (publication ready)\n",
    "# -------------------------\n",
    "class PublicationVisualizer:\n",
    "\n",
    "    @staticmethod\n",
    "    def remap_labels(labels):\n",
    "        return np.array([ORIGINAL_TO_NEW[int(label)] for label in labels])\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(y_true, y_pred, output_dir, filename):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "        y_pred_r = PublicationVisualizer.remap_labels(y_pred)\n",
    "        cm = confusion_matrix(y_true_r, y_pred_r)\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "                    cbar=False, annot_kws={\"size\": 22, \"fontweight\": \"bold\"})\n",
    "        plt.xlabel('Predicted Label', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Label', fontsize=18, fontweight='bold')\n",
    "        plt.setp(plt.gca().get_xticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.setp(plt.gca().get_yticklabels(), fontweight='bold', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_roc_curves(y_true, y_proba, output_dir, filename):\n",
    "        y_true_r = PublicationVisualizer.remap_labels(y_true)\n",
    "\n",
    "        y_proba_r = np.zeros_like(y_proba)\n",
    "        for old_idx, new_idx in ORIGINAL_TO_NEW.items():\n",
    "            y_proba_r[:, new_idx] = y_proba[:, old_idx]\n",
    "\n",
    "        y_bin = label_binarize(y_true_r, classes=[0, 1, 2, 3])\n",
    "        fpr, tpr, roc_auc = {}, {}, {}\n",
    "        for i in range(4):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_proba_r[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        plt.figure(figsize=(7, 6))\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "        line_styles = ['-', '--', '-.', ':']\n",
    "        for i in range(4):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2.5, color=colors[i], linestyle=line_styles[i],\n",
    "                     label=f'{CLASS_NAMES[i]} (AUC = {roc_auc[i]:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', alpha=0.5)\n",
    "        plt.xlabel('False Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.ylabel('True Positive Rate', fontsize=18, fontweight='bold')\n",
    "        plt.legend(loc='lower right', fontsize=13, frameon=True, framealpha=0.95)\n",
    "        plt.grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        plt.xticks(fontsize=14, fontweight='bold')\n",
    "        plt.yticks(fontsize=14, fontweight='bold')\n",
    "        plt.xlim([-0.02, 1.02])\n",
    "        plt.ylim([-0.02, 1.02])\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=1000, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_curves(history, output_dir, filename):\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "        axes[0].plot(epochs, history['train_loss'], label='Train', linewidth=2.5)\n",
    "        axes[0].plot(epochs, history['val_loss'], label='Validation', linewidth=2.5)\n",
    "        axes[0].set_title('Token-level Loss', fontsize=14, fontweight='bold')\n",
    "        axes[0].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[0].legend()\n",
    "\n",
    "        axes[1].plot(epochs, history['train_acc'], label='Train', linewidth=2.5)\n",
    "        axes[1].plot(epochs, history['val_acc'], label='Validation', linewidth=2.5)\n",
    "        axes[1].set_title('Token-level Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[1].legend()\n",
    "\n",
    "        axes[2].plot(epochs, history['val_sample_acc'], label='Val SAMPLE', linewidth=2.5)\n",
    "        axes[2].set_title('VAL SAMPLE Accuracy', fontsize=14, fontweight='bold')\n",
    "        axes[2].grid(alpha=0.3, linestyle='--', linewidth=0.8)\n",
    "        axes[2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def plot_lowdata_curve(fracs, accs, out_dir, filename=\"lowdata_curve.png\"):\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot([f*100 for f in fracs], [a*100 for a in accs], marker='o', linewidth=2.5)\n",
    "    plt.xlabel(\"Train files per class (%)\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"Mean CV Accuracy (%)\", fontsize=14, fontweight='bold')\n",
    "    plt.grid(alpha=0.3, linestyle='--')\n",
    "    plt.xticks(fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(out_dir, filename), dpi=600, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SIGNAL + FEATURES\n",
    "# -------------------------\n",
    "def robust_mad(x):\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) + 1e-12\n",
    "    return med, mad\n",
    "\n",
    "def find_1d_signal_in_mat(mat_dict):\n",
    "    candidates = []\n",
    "    for k, v in mat_dict.items():\n",
    "        if k.startswith(\"__\"):\n",
    "            continue\n",
    "        if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "            arr = np.array(v).squeeze()\n",
    "            if arr.ndim == 1 and arr.size > 1000:\n",
    "                candidates.append((k, arr.size, arr))\n",
    "    if not candidates:\n",
    "        for k, v in mat_dict.items():\n",
    "            if k.startswith(\"__\"):\n",
    "                continue\n",
    "            if isinstance(v, np.ndarray) and np.issubdtype(v.dtype, np.number):\n",
    "                arr = np.array(v).squeeze()\n",
    "                if arr.size > 1000:\n",
    "                    return k, arr.reshape(-1).astype(np.float32)\n",
    "        raise ValueError(\"No suitable numeric signal array found in .mat file.\")\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[0][0], candidates[0][2].astype(np.float32)\n",
    "\n",
    "def short_time_energy(x, win):\n",
    "    x2 = x.astype(np.float64) ** 2\n",
    "    kernel = np.ones(win, dtype=np.float64)\n",
    "    return np.convolve(x2, kernel, mode=\"same\").astype(np.float32)\n",
    "\n",
    "def energy_peak_tokenize(x, energy_win, k_mad, peak_distance, seg_len, max_tokens):\n",
    "    if x.size < seg_len + 10:\n",
    "        return []\n",
    "    E = short_time_energy(x, energy_win)\n",
    "    med, mad = robust_mad(E)\n",
    "    thr = med + k_mad * mad\n",
    "\n",
    "    peaks, props = find_peaks(E, height=thr, distance=peak_distance)\n",
    "    if peaks.size == 0:\n",
    "        mid = x.size // 2\n",
    "        half = seg_len // 2\n",
    "        seg = x[max(0, mid-half): min(x.size, mid+half)]\n",
    "        return [seg.astype(np.float32)] if seg.size == seg_len else []\n",
    "\n",
    "    heights = props.get(\"peak_heights\", E[peaks])\n",
    "    order = np.argsort(heights)[::-1]\n",
    "    peaks = peaks[order][:max_tokens]\n",
    "\n",
    "    half = seg_len // 2\n",
    "    tokens = []\n",
    "    for p in peaks:\n",
    "        s = p - half\n",
    "        e = p + half\n",
    "        if s < 0 or e > x.size:\n",
    "            continue\n",
    "        seg = x[s:e].astype(np.float32)\n",
    "        if seg.size == seg_len:\n",
    "            tokens.append(seg)\n",
    "    return tokens\n",
    "\n",
    "def time_features(seg):\n",
    "    x = seg.astype(np.float64)\n",
    "    x0 = x - np.mean(x)\n",
    "    rms = np.sqrt(np.mean(x0**2) + 1e-12)\n",
    "    peak = np.max(np.abs(x0)) + 1e-12\n",
    "    ptp = np.ptp(x0)\n",
    "    crest = peak / (rms + 1e-12)\n",
    "    kurt = kurtosis(x0, fisher=False, bias=False) if x0.size > 10 else 0.0\n",
    "    sk = skew(x0, bias=False) if x0.size > 10 else 0.0\n",
    "    return np.array([rms, peak, ptp, crest, kurt, sk], dtype=np.float32)\n",
    "\n",
    "def spectral_features(seg, fft_n, eps=1e-12):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    n = min(len(x), fft_n)\n",
    "    w = np.hanning(n)\n",
    "    xw = x[:n] * w\n",
    "    X = np.fft.rfft(xw, n=n)\n",
    "    mag = np.abs(X) + eps\n",
    "    psd = mag**2 + eps\n",
    "    freqs = np.fft.rfftfreq(n, d=1.0)\n",
    "\n",
    "    psd_sum = np.sum(psd) + eps\n",
    "    p = psd / psd_sum\n",
    "\n",
    "    centroid = np.sum(freqs * psd) / psd_sum\n",
    "    bandwidth = np.sqrt(np.sum(((freqs - centroid) ** 2) * psd) / psd_sum)\n",
    "    entropy = -np.sum(p * np.log(p + eps))\n",
    "    dom_freq = freqs[int(np.argmax(psd))]\n",
    "    rolloff_85 = freqs[np.searchsorted(np.cumsum(psd) / psd_sum, 0.85)]\n",
    "\n",
    "    flatness = np.exp(np.mean(np.log(psd))) / (np.mean(psd) + eps)\n",
    "    psd_kurt = kurtosis(psd, fisher=False, bias=False) if psd.size > 10 else 0.0\n",
    "\n",
    "    return np.array([centroid, bandwidth, entropy, dom_freq, rolloff_85, flatness, psd_kurt], dtype=np.float32)\n",
    "\n",
    "def wpt_topk_energy(seg, wavelet, level, topk):\n",
    "    x = seg.astype(np.float64)\n",
    "    x = x - np.mean(x)\n",
    "    wp = pywt.WaveletPacket(data=x, wavelet=wavelet, mode=\"symmetric\", maxlevel=level)\n",
    "    nodes = wp.get_level(level, order=\"freq\")\n",
    "    energies = np.array([np.sum(n.data**2) for n in nodes], dtype=np.float64)\n",
    "    energies = energies / (np.sum(energies) + 1e-12)\n",
    "    top = np.sort(energies)[::-1][:topk]\n",
    "    return top.astype(np.float32)\n",
    "\n",
    "def extract_features_from_token(seg, cfg):\n",
    "    tf = time_features(seg)\n",
    "    sf = spectral_features(seg, fft_n=cfg[\"FFT_N\"])\n",
    "    wf = wpt_topk_energy(seg, wavelet=cfg[\"WPT_WAVELET\"], level=cfg[\"WPT_LEVEL\"], topk=cfg[\"TOPK_WPT\"])\n",
    "    return np.concatenate([tf, sf, wf], axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TORCH DATA + MODEL\n",
    "# -------------------------\n",
    "class TokenDataset(Dataset):\n",
    "    def __init__(self, X, y, sid):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.sid = torch.tensor(sid, dtype=torch.long)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.sid[idx]\n",
    "\n",
    "class EmbNet(nn.Module):\n",
    "    def __init__(self, in_dim, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, emb_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        return F.normalize(z, dim=1)\n",
    "\n",
    "def compute_prototypes(model, loader, num_classes, emb_dim):\n",
    "    model.eval()\n",
    "    sums = torch.zeros((num_classes, emb_dim), device=DEVICE)\n",
    "    counts = torch.zeros((num_classes,), device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "            z = model(xb)\n",
    "            for c in range(num_classes):\n",
    "                m = (yb == c)\n",
    "                if m.any():\n",
    "                    sums[c] += z[m].sum(dim=0)\n",
    "                    counts[c] += m.sum()\n",
    "    protos = sums / (counts.unsqueeze(1) + 1e-12)\n",
    "    return F.normalize(protos, dim=1)\n",
    "\n",
    "def logits_from_prototypes(z, protos):\n",
    "    return z @ protos.t()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# SAMPLE-LEVEL EVAL\n",
    "# -------------------------\n",
    "def sample_level_eval(model, loader, sids_set, sample_true_dict, protos, num_classes, alphas):\n",
    "    model.eval()\n",
    "\n",
    "    tok_sid = []\n",
    "    tok_pred_major = []\n",
    "    tok_prob_by_alpha = {a: [] for a in alphas}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, _, sidb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            tok_pred_major.extend(pred.tolist())\n",
    "            tok_sid.extend(sidb.numpy().tolist())\n",
    "\n",
    "            for a in alphas:\n",
    "                probs = torch.softmax(a * logits, dim=1).cpu().numpy()\n",
    "                tok_prob_by_alpha[a].append(probs)\n",
    "\n",
    "    tok_sid = np.array(tok_sid, dtype=np.int64)\n",
    "    tok_pred_major = np.array(tok_pred_major, dtype=np.int64)\n",
    "\n",
    "    votes = {sid: [] for sid in sids_set}\n",
    "    for sid, p in zip(tok_sid, tok_pred_major):\n",
    "        if sid in votes:\n",
    "            votes[sid].append(p)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred_major = []\n",
    "    for sid in sorted(votes.keys()):\n",
    "        y_true.append(sample_true_dict[sid])\n",
    "        vals = votes[sid]\n",
    "        if len(vals) == 0:\n",
    "            y_pred_major.append(0)\n",
    "        else:\n",
    "            u, c = np.unique(vals, return_counts=True)\n",
    "            y_pred_major.append(int(u[np.argmax(c)]))\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.int64)\n",
    "    y_pred_major = np.array(y_pred_major, dtype=np.int64)\n",
    "    acc_major = float((y_true == y_pred_major).mean())\n",
    "\n",
    "    proba_major = np.zeros((len(y_pred_major), num_classes), dtype=np.float32)\n",
    "    for i,p in enumerate(y_pred_major):\n",
    "        proba_major[i,p] = 1.0\n",
    "\n",
    "    best = {\"mode\":\"majority\", \"alpha\":None, \"acc\":acc_major,\n",
    "            \"y_true\":y_true, \"y_pred\":y_pred_major, \"y_proba\":proba_major}\n",
    "\n",
    "    for a in alphas:\n",
    "        probs_all = np.concatenate(tok_prob_by_alpha[a], axis=0)\n",
    "        scores = {sid: np.zeros((num_classes,), dtype=np.float64) for sid in sids_set}\n",
    "        for sid, pv in zip(tok_sid, probs_all):\n",
    "            if sid in scores:\n",
    "                scores[sid] += pv\n",
    "\n",
    "        y_pred = []\n",
    "        y_proba = []\n",
    "        for sid in sorted(scores.keys()):\n",
    "            sc = scores[sid]\n",
    "            y_pred.append(int(np.argmax(sc)))\n",
    "            y_proba.append((sc / (np.sum(sc) + 1e-12)).astype(np.float32))\n",
    "\n",
    "        y_pred = np.array(y_pred, dtype=np.int64)\n",
    "        acc = float((y_true == y_pred).mean())\n",
    "        if acc > best[\"acc\"]:\n",
    "            best = {\"mode\":\"weighted\", \"alpha\":float(a), \"acc\":acc,\n",
    "                    \"y_true\":y_true, \"y_pred\":y_pred, \"y_proba\":np.stack(y_proba, axis=0)}\n",
    "    return best\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# BUILD FILE LISTS\n",
    "# -------------------------\n",
    "def collect_files_by_class(class_dirs):\n",
    "    all_files = []\n",
    "    all_labels = []\n",
    "    for cname in CLASS_NAMES:\n",
    "        files = sorted(glob.glob(os.path.join(class_dirs[cname], \"*.mat\")))\n",
    "        all_files.extend(files)\n",
    "        all_labels.extend([cname]*len(files))\n",
    "    return np.array(all_files), np.array(all_labels)\n",
    "\n",
    "def build_token_dataset_from_files(files, labels, label_map, cfg, sid_start=0):\n",
    "    all_X, all_y, all_sid = [], [], []\n",
    "    sample_meta = []  # (sid, cls, filepath)\n",
    "    sid = sid_start\n",
    "\n",
    "    for fp, cls in zip(files, labels):\n",
    "        try:\n",
    "            md = loadmat(fp)\n",
    "            _, sig = find_1d_signal_in_mat(md)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        tokens = energy_peak_tokenize(\n",
    "            sig,\n",
    "            energy_win=cfg[\"ENERGY_WIN\"],\n",
    "            k_mad=cfg[\"K_MAD\"],\n",
    "            peak_distance=cfg[\"PEAK_DISTANCE\"],\n",
    "            seg_len=cfg[\"SEG_LEN\"],\n",
    "            max_tokens=cfg[\"MAX_TOKENS_PER_FILE\"]\n",
    "        )\n",
    "        if len(tokens) == 0:\n",
    "            continue\n",
    "\n",
    "        sample_meta.append((sid, cls, fp))\n",
    "        for seg in tokens:\n",
    "            all_X.append(extract_features_from_token(seg, cfg))\n",
    "            all_y.append(label_map[cls])\n",
    "            all_sid.append(sid)\n",
    "        sid += 1\n",
    "\n",
    "    return (np.stack(all_X, axis=0),\n",
    "            np.array(all_y, dtype=np.int64),\n",
    "            np.array(all_sid, dtype=np.int64),\n",
    "            sample_meta)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# TRAIN+EVAL ONE SPLIT (train files / val files / test files)\n",
    "# -------------------------\n",
    "def train_eval_split(rpm_name, out_dir, train_files, train_labels, val_files, val_labels, test_files, test_labels, cfg):\n",
    "    global ORIGINAL_TO_NEW\n",
    "\n",
    "    class_order = CLASS_NAMES[:]  # enforce fixed internal order\n",
    "    label_map = {c:i for i,c in enumerate(class_order)}\n",
    "    ORIGINAL_TO_NEW = {label_map[c]: CLASS_NAMES.index(c) for c in CLASS_NAMES}\n",
    "\n",
    "    # Build token datasets\n",
    "    X_tr, y_tr, sid_tr, meta_tr = build_token_dataset_from_files(train_files, train_labels, label_map, cfg, sid_start=0)\n",
    "    X_va, y_va, sid_va, meta_va = build_token_dataset_from_files(val_files,   val_labels,   label_map, cfg, sid_start=10_000_000)\n",
    "    X_te, y_te, sid_te, meta_te = build_token_dataset_from_files(test_files,  test_labels,  label_map, cfg, sid_start=20_000_000)\n",
    "\n",
    "    # Sample true dicts (by sid)\n",
    "    val_true  = {sid: label_map[cls] for sid, cls, _ in meta_va}\n",
    "    test_true = {sid: label_map[cls] for sid, cls, _ in meta_te}\n",
    "\n",
    "    # Standardize (train only)\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr).astype(np.float32)\n",
    "    X_va = scaler.transform(X_va).astype(np.float32)\n",
    "    X_te = scaler.transform(X_te).astype(np.float32)\n",
    "\n",
    "    # Loaders\n",
    "    train_ds = TokenDataset(X_tr, y_tr, sid_tr)\n",
    "    val_ds   = TokenDataset(X_va, y_va, sid_va)\n",
    "    test_ds  = TokenDataset(X_te, y_te, sid_te)\n",
    "\n",
    "    if cfg[\"USE_BALANCED_SAMPLER\"]:\n",
    "        counts = np.bincount(y_tr, minlength=len(class_order)).astype(np.float64)\n",
    "        cw = 1.0 / (counts + 1e-12)\n",
    "        sw = cw[y_tr]\n",
    "        sampler = WeightedRandomSampler(sw, num_samples=len(sw), replacement=True)\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, drop_last=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "\n",
    "    val_loader  = DataLoader(val_ds,  batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "\n",
    "    # Model\n",
    "    emb_dim = 64\n",
    "    model = EmbNet(in_dim=X_tr.shape[1], emb_dim=emb_dim).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=cfg[\"WEIGHT_DECAY\"])\n",
    "\n",
    "    num_classes = len(class_order)\n",
    "\n",
    "    history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[], \"val_sample_acc\":[]}\n",
    "    best_state = None\n",
    "    best_val_sample = -1.0\n",
    "    best_choice = None\n",
    "\n",
    "    def token_eval_loss_acc(loader, protos):\n",
    "        model.eval()\n",
    "        total_loss, total_correct, n = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, _ in loader:\n",
    "                xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "                z = model(xb)\n",
    "                logits = logits_from_prototypes(z, protos)\n",
    "                ce = F.cross_entropy(logits, yb)\n",
    "                p_y = protos[yb]\n",
    "                comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "                loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "                bs = xb.size(0)\n",
    "                total_loss += loss.item() * bs\n",
    "                total_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "                n += bs\n",
    "        return total_loss/max(n,1), 100.0*(total_correct/max(n,1))\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                    num_classes=num_classes, emb_dim=emb_dim)\n",
    "\n",
    "        model.train()\n",
    "        run_loss, run_correct, seen = 0.0, 0, 0\n",
    "        for xb, yb, _ in train_loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE)\n",
    "            z = model(xb)\n",
    "            logits = logits_from_prototypes(z, protos)\n",
    "\n",
    "            ce = F.cross_entropy(logits, yb)\n",
    "            p_y = protos[yb]\n",
    "            comp = ((z - p_y)**2).sum(dim=1).mean()\n",
    "            loss = ce + LAMBDA_COMPACT * comp\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            bs = xb.size(0)\n",
    "            run_loss += loss.item() * bs\n",
    "            run_correct += (torch.argmax(logits, dim=1) == yb).sum().item()\n",
    "            seen += bs\n",
    "\n",
    "        train_loss = run_loss/max(seen,1)\n",
    "        train_acc = 100.0*(run_correct/max(seen,1))\n",
    "        val_loss, val_acc = token_eval_loss_acc(val_loader, protos)\n",
    "\n",
    "        val_choice = sample_level_eval(model, val_loader, set([m[0] for m in meta_va]), val_true,\n",
    "                                       protos, num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "        val_sample_acc = 100.0 * val_choice[\"acc\"]\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_sample_acc\"].append(val_sample_acc)\n",
    "\n",
    "        if epoch >= MIN_EPOCH_SAVE and val_choice[\"acc\"] > best_val_sample:\n",
    "            best_val_sample = val_choice[\"acc\"]\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            best_choice = {\"epoch\": epoch, \"mode\": val_choice[\"mode\"], \"alpha\": val_choice[\"alpha\"],\n",
    "                           \"val_sample_acc\": float(val_sample_acc)}\n",
    "\n",
    "    # fallback if best never updated (e.g., EPOCHS < MIN_EPOCH_SAVE)\n",
    "    if best_state is None:\n",
    "        best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        best_choice = {\"epoch\": EPOCHS, \"mode\": \"majority\", \"alpha\": None, \"val_sample_acc\": float(history[\"val_sample_acc\"][-1])}\n",
    "\n",
    "    model.load_state_dict({k: v.to(DEVICE) for k,v in best_state.items()})\n",
    "    protos = compute_prototypes(model, DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=False),\n",
    "                                num_classes=num_classes, emb_dim=emb_dim)\n",
    "\n",
    "    # TEST sample-level\n",
    "    test_sids_set = set([m[0] for m in meta_te])\n",
    "    test_choice = sample_level_eval(model, test_loader, test_sids_set, test_true,\n",
    "                                    protos, num_classes=num_classes, alphas=ALPHA_GRID)\n",
    "\n",
    "    y_true = test_choice[\"y_true\"]\n",
    "    y_pred = test_choice[\"y_pred\"]\n",
    "    y_proba = test_choice[\"y_proba\"]\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=class_order, digits=4)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Save artifacts\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    PublicationVisualizer.plot_training_curves(history, out_dir, f\"{rpm_name}_training_curves.png\")\n",
    "    PublicationVisualizer.plot_confusion_matrix(y_true, y_pred, out_dir, f\"{rpm_name}_CM_SAMPLE.png\")\n",
    "    PublicationVisualizer.plot_roc_curves(y_true, y_proba, out_dir, f\"{rpm_name}_ROC_SAMPLE.png\")\n",
    "\n",
    "    with open(os.path.join(out_dir, f\"{rpm_name}_report.txt\"), \"w\") as f:\n",
    "        f.write(\"Best choice:\\n\" + json.dumps(best_choice, indent=2) + \"\\n\\n\")\n",
    "        f.write(\"Test choice:\\n\" + json.dumps({k:test_choice[k] for k in [\"mode\",\"alpha\",\"acc\"]}, indent=2) + \"\\n\\n\")\n",
    "        f.write(report + \"\\n\\n\")\n",
    "        f.write(\"Confusion matrix:\\n\" + np.array2string(cm) + \"\\n\")\n",
    "\n",
    "    np.savez(os.path.join(out_dir, f\"{rpm_name}_outputs.npz\"),\n",
    "             y_true=y_true, y_pred=y_pred, y_proba=y_proba)\n",
    "\n",
    "    return {\n",
    "        \"acc\": float(test_choice[\"acc\"]),\n",
    "        \"macro_f1\": float(np.mean([float(x) for x in report.split() if x.replace('.','',1).isdigit()]) if False else np.nan),  # not used\n",
    "        \"best_choice\": best_choice,\n",
    "        \"test_choice\": {k:test_choice[k] for k in [\"mode\",\"alpha\",\"acc\"]},\n",
    "        \"cm\": cm\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5-FOLD CV (file-level, stratified)\n",
    "# -------------------------\n",
    "def run_cv5(rpm_name, class_dirs, base_out_dir, cfg):\n",
    "    files, labels = collect_files_by_class(class_dirs)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    cv_dir = os.path.join(base_out_dir, \"CV5\")\n",
    "    os.makedirs(cv_dir, exist_ok=True)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_accs = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(files, labels), start=1):\n",
    "        fold_dir = os.path.join(cv_dir, f\"Fold_{fold}\")\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "        train_files_all = files[train_idx]\n",
    "        train_labels_all = labels[train_idx]\n",
    "        test_files = files[test_idx]\n",
    "        test_labels = labels[test_idx]\n",
    "\n",
    "        # inner val split on train-fold (file-level)\n",
    "        tr_f, va_f, tr_l, va_l = train_test_split(\n",
    "            train_files_all, train_labels_all,\n",
    "            test_size=VAL_SIZE_FROM_TRAIN,\n",
    "            random_state=SEED,\n",
    "            stratify=train_labels_all\n",
    "        )\n",
    "\n",
    "        res = train_eval_split(\n",
    "            rpm_name=f\"{rpm_name}_Fold{fold}\",\n",
    "            out_dir=fold_dir,\n",
    "            train_files=tr_f, train_labels=tr_l,\n",
    "            val_files=va_f, val_labels=va_l,\n",
    "            test_files=test_files, test_labels=test_labels,\n",
    "            cfg=cfg\n",
    "        )\n",
    "        fold_accs.append(res[\"acc\"])\n",
    "        fold_results.append({\n",
    "            \"fold\": fold,\n",
    "            \"acc\": res[\"acc\"],\n",
    "            \"best_choice\": res[\"best_choice\"],\n",
    "            \"test_choice\": res[\"test_choice\"]\n",
    "        })\n",
    "        print(f\"[{rpm_name}] Fold {fold}/{N_FOLDS} -> TEST sample-acc: {res['acc']*100:.2f}%\")\n",
    "\n",
    "    mean_acc = float(np.mean(fold_accs))\n",
    "    std_acc = float(np.std(fold_accs))\n",
    "\n",
    "    # Save summary\n",
    "    summary = {\n",
    "        \"rpm\": rpm_name,\n",
    "        \"n_folds\": N_FOLDS,\n",
    "        \"fold_accs\": fold_accs,\n",
    "        \"mean_acc\": mean_acc,\n",
    "        \"std_acc\": std_acc,\n",
    "        \"cfg\": cfg,\n",
    "        \"fold_details\": fold_results\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(cv_dir, \"CV5_summary.json\"), \"w\") as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    # CSV\n",
    "    csv_path = os.path.join(cv_dir, \"CV5_summary.csv\")\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(\"fold,acc\\n\")\n",
    "        for i,a in enumerate(fold_accs, start=1):\n",
    "            f.write(f\"{i},{a:.6f}\\n\")\n",
    "        f.write(f\"mean,{mean_acc:.6f}\\n\")\n",
    "        f.write(f\"std,{std_acc:.6f}\\n\")\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] CV5 mean acc = {mean_acc*100:.2f}% ± {std_acc*100:.2f}%\")\n",
    "    print(f\"[{rpm_name}] Saved CV5 to: {cv_dir}\")\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LOW-DATA PROTOCOL (uses CV folds; subsample train-fold per class)\n",
    "# -------------------------\n",
    "def subsample_train_per_class(files, labels, frac, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    out_files, out_labels = [], []\n",
    "    for c in CLASS_NAMES:\n",
    "        idx = np.where(labels == c)[0]\n",
    "        n = len(idx)\n",
    "        k = max(1, int(math.ceil(frac * n)))\n",
    "        pick = rng.choice(idx, size=k, replace=False)\n",
    "        out_files.extend(files[pick].tolist())\n",
    "        out_labels.extend(labels[pick].tolist())\n",
    "    return np.array(out_files), np.array(out_labels)\n",
    "\n",
    "def run_lowdata_cv(rpm_name, class_dirs, base_out_dir, cfg):\n",
    "    files, labels = collect_files_by_class(class_dirs)\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    out_dir = os.path.join(base_out_dir, \"LowData\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    frac_to_mean = {}\n",
    "    frac_to_std = {}\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for frac in LOWDATA_FRACS:\n",
    "        fold_accs = []\n",
    "        for fold, (train_idx, test_idx) in enumerate(skf.split(files, labels), start=1):\n",
    "            train_files_all = files[train_idx]\n",
    "            train_labels_all = labels[train_idx]\n",
    "            test_files = files[test_idx]\n",
    "            test_labels = labels[test_idx]\n",
    "\n",
    "            # inner val split\n",
    "            tr_f_all, va_f, tr_l_all, va_l = train_test_split(\n",
    "                train_files_all, train_labels_all,\n",
    "                test_size=VAL_SIZE_FROM_TRAIN,\n",
    "                random_state=SEED,\n",
    "                stratify=train_labels_all\n",
    "            )\n",
    "\n",
    "            # subsample ONLY training part per class\n",
    "            tr_f, tr_l = subsample_train_per_class(tr_f_all, tr_l_all, frac, seed=SEED+fold+int(frac*1000))\n",
    "\n",
    "            fold_dir = os.path.join(out_dir, f\"frac_{int(frac*100)}\", f\"Fold_{fold}\")\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "            res = train_eval_split(\n",
    "                rpm_name=f\"{rpm_name}_LD{int(frac*100)}_Fold{fold}\",\n",
    "                out_dir=fold_dir,\n",
    "                train_files=tr_f, train_labels=tr_l,\n",
    "                val_files=va_f, val_labels=va_l,\n",
    "                test_files=test_files, test_labels=test_labels,\n",
    "                cfg=cfg\n",
    "            )\n",
    "            fold_accs.append(res[\"acc\"])\n",
    "            all_rows.append({\"frac\": frac, \"fold\": fold, \"acc\": res[\"acc\"]})\n",
    "            print(f\"[{rpm_name}] LowData {int(frac*100)}% | Fold {fold} -> {res['acc']*100:.2f}%\")\n",
    "\n",
    "        frac_to_mean[frac] = float(np.mean(fold_accs))\n",
    "        frac_to_std[frac] = float(np.std(fold_accs))\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(out_dir, \"lowdata_summary.csv\")\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        f.write(\"frac,fold,acc\\n\")\n",
    "        for r in all_rows:\n",
    "            f.write(f\"{r['frac']:.2f},{r['fold']},{r['acc']:.6f}\\n\")\n",
    "        f.write(\"\\nfrac,mean,std\\n\")\n",
    "        for frac in LOWDATA_FRACS:\n",
    "            f.write(f\"{frac:.2f},{frac_to_mean[frac]:.6f},{frac_to_std[frac]:.6f}\\n\")\n",
    "\n",
    "    # Plot curve\n",
    "    fracs = LOWDATA_FRACS\n",
    "    means = [frac_to_mean[f] for f in fracs]\n",
    "    plot_lowdata_curve(fracs, means, out_dir, filename=\"lowdata_curve.png\")\n",
    "\n",
    "    with open(os.path.join(out_dir, \"lowdata_summary.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"rpm\": rpm_name,\n",
    "            \"fracs\": fracs,\n",
    "            \"mean_acc\": {str(k): v for k,v in frac_to_mean.items()},\n",
    "            \"std_acc\": {str(k): v for k,v in frac_to_std.items()},\n",
    "            \"cfg\": cfg\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(f\"\\n[{rpm_name}] Saved LowData results to: {out_dir}\")\n",
    "    return {\"mean\": frac_to_mean, \"std\": frac_to_std}\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PATHS\n",
    "# -------------------------\n",
    "BASE_660 = r\"F:\\20240925\"\n",
    "DIRS_660 = {\n",
    "    \"BF\": os.path.join(BASE_660, \"BF660_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_660, \"GF660_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_660, \"TF660_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_660, \"N660_1\",  \"AE\"),\n",
    "}\n",
    "OUT_660 = r\"E:\\Conferences Umar\\Conference 3\\Results\\660_RPM_Final\"\n",
    "\n",
    "BASE_720 = r\"F:\\D4B2\\720\"\n",
    "DIRS_720 = {\n",
    "    \"BF\": os.path.join(BASE_720, \"BF720_1\", \"AE\"),\n",
    "    \"GF\": os.path.join(BASE_720, \"GF720_1\", \"AE\"),\n",
    "    \"TF\": os.path.join(BASE_720, \"TF720_1\", \"AE\"),\n",
    "    \"N\":  os.path.join(BASE_720, \"N720_1\",  \"AE\"),\n",
    "}\n",
    "OUT_720 = r\"E:\\Conferences Umar\\Conference 3\\Results\\720_RPM_Final\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FINAL CONFIGS (your working ones)\n",
    "# -------------------------\n",
    "CFG_660 = dict(\n",
    "    ENERGY_WIN=256,\n",
    "    K_MAD=6.0,\n",
    "    PEAK_DISTANCE=800,\n",
    "    SEG_LEN=4096,\n",
    "    MAX_TOKENS_PER_FILE=200,\n",
    "    FFT_N=2048,\n",
    "    WPT_WAVELET=\"db4\",\n",
    "    WPT_LEVEL=5,\n",
    "    TOPK_WPT=10,\n",
    "    WEIGHT_DECAY=0.0,\n",
    "    USE_BALANCED_SAMPLER=False\n",
    ")\n",
    "\n",
    "CFG_720 = dict(\n",
    "    ENERGY_WIN=256,\n",
    "    K_MAD=5.0,\n",
    "    PEAK_DISTANCE=600,\n",
    "    SEG_LEN=4096,\n",
    "    MAX_TOKENS_PER_FILE=350,\n",
    "    FFT_N=2048,\n",
    "    WPT_WAVELET=\"db4\",\n",
    "    WPT_LEVEL=5,\n",
    "    TOPK_WPT=16,\n",
    "    WEIGHT_DECAY=1e-4,\n",
    "    USE_BALANCED_SAMPLER=True\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# RUN: CV5 + LOWDATA for BOTH RPMs\n",
    "# -------------------------\n",
    "print(\"\\n================= 660 RPM: CV5 =================\")\n",
    "cv660 = run_cv5(\"660_RPM\", DIRS_660, OUT_660, CFG_660)\n",
    "\n",
    "print(\"\\n================= 660 RPM: LowData =================\")\n",
    "ld660 = run_lowdata_cv(\"660_RPM\", DIRS_660, OUT_660, CFG_660)\n",
    "\n",
    "print(\"\\n================= 720 RPM: CV5 =================\")\n",
    "cv720 = run_cv5(\"720_RPM\", DIRS_720, OUT_720, CFG_720)\n",
    "\n",
    "print(\"\\n================= 720 RPM: LowData =================\")\n",
    "ld720 = run_lowdata_cv(\"720_RPM\", DIRS_720, OUT_720, CFG_720)\n",
    "\n",
    "print(\"\\n✅ ALL DONE. Results saved under:\")\n",
    "print(\" -\", os.path.join(OUT_660, \"CV5\"), \"and\", os.path.join(OUT_660, \"LowData\"))\n",
    "print(\" -\", os.path.join(OUT_720, \"CV5\"), \"and\", os.path.join(OUT_720, \"LowData\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40d2bb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GENERATING PROPOSED METHOD ARCHITECTURE DIAGRAM\n",
      "================================================================================\n",
      "\n",
      "1. Generating complete architecture overview...\n",
      "  ✓ Saved: complete_architecture.png\n",
      "\n",
      "2. Generating detailed MAE architecture...\n",
      "  ✓ Saved: mae_detailed_architecture.png\n",
      "\n",
      "3. Generating detailed MIL architecture...\n",
      "  ✓ Saved: mil_detailed_architecture.png\n",
      "\n",
      "4. Generating two-stage training pipeline...\n",
      "  ✓ Saved: two_stage_training_pipeline.png\n",
      "\n",
      "5. Generating simplified workflow diagram...\n",
      "  ✓ Saved: simplified_workflow.png\n",
      "\n",
      "================================================================================\n",
      "✅ ARCHITECTURE DIAGRAMS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "Generated diagrams:\n",
      "  1. complete_architecture.png           - Full method overview\n",
      "  2. mae_detailed_architecture.png       - MAE component details\n",
      "  3. mil_detailed_architecture.png       - MIL component details\n",
      "  4. two_stage_training_pipeline.png     - Training procedure\n",
      "  5. simplified_workflow.png             - High-level workflow\n",
      "\n",
      "All saved to: E:\\Conferences Umar\\Conference 3\\Results\\Architecture_Diagram\n",
      "Resolution: 1200 DPI, publication-ready\n",
      "Format: PNG with white background\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROPOSED METHOD ARCHITECTURE DIAGRAM\n",
    "# BurstMAE-MIL: Self-Supervised Masked Autoencoder + Multiple Instance Learning\n",
    "# Complete pipeline visualization for publication\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch, Rectangle, Circle, Wedge\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.path import Path\n",
    "import matplotlib.patheffects as path_effects\n",
    "\n",
    "# -------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------\n",
    "\n",
    "OUTPUT_DIR = r\"E:\\Conferences Umar\\Conference 3\\Results\\Architecture_Diagram\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Color scheme\n",
    "COLORS = {\n",
    "    'data': '#E3F2FD',          # Light blue - data\n",
    "    'preprocessing': '#FFF3E0',  # Light orange - preprocessing\n",
    "    'mae': '#F3E5F5',           # Light purple - MAE\n",
    "    'mil': '#E8F5E9',           # Light green - MIL\n",
    "    'output': '#FFEBEE',        # Light red - output\n",
    "    'arrow': '#424242',         # Dark gray - arrows\n",
    "    'text': '#212121',          # Black - text\n",
    "    'accent1': '#1976D2',       # Blue - primary\n",
    "    'accent2': '#F57C00',       # Orange - secondary\n",
    "    'accent3': '#7B1FA2',       # Purple - tertiary\n",
    "    'accent4': '#388E3C',       # Green - quaternary\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GENERATING PROPOSED METHOD ARCHITECTURE DIAGRAM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -------------------------\n",
    "# HELPER FUNCTIONS FOR DRAWING\n",
    "# -------------------------\n",
    "\n",
    "def draw_box(ax, x, y, width, height, text, color, text_size=10, bold=True, edge_color='black', edge_width=2):\n",
    "    \"\"\"Draw a fancy box with text\"\"\"\n",
    "    box = FancyBboxPatch(\n",
    "        (x, y), width, height,\n",
    "        boxstyle=\"round,pad=0.05\",\n",
    "        facecolor=color,\n",
    "        edgecolor=edge_color,\n",
    "        linewidth=edge_width,\n",
    "        alpha=0.9\n",
    "    )\n",
    "    ax.add_patch(box)\n",
    "    \n",
    "    # Add text\n",
    "    weight = 'bold' if bold else 'normal'\n",
    "    txt = ax.text(x + width/2, y + height/2, text,\n",
    "                 ha='center', va='center',\n",
    "                 fontsize=text_size, fontweight=weight,\n",
    "                 color=COLORS['text'])\n",
    "    \n",
    "    # Add white outline for better readability\n",
    "    txt.set_path_effects([path_effects.withStroke(linewidth=3, foreground='white')])\n",
    "    \n",
    "    return box\n",
    "\n",
    "def draw_arrow(ax, x1, y1, x2, y2, text='', color='black', style='simple', width=2, text_size=9):\n",
    "    \"\"\"Draw arrow with optional label\"\"\"\n",
    "    arrow = FancyArrowPatch(\n",
    "        (x1, y1), (x2, y2),\n",
    "        arrowstyle='->' if style == 'simple' else 'fancy',\n",
    "        mutation_scale=20,\n",
    "        linewidth=width,\n",
    "        color=color,\n",
    "        zorder=1\n",
    "    )\n",
    "    ax.add_patch(arrow)\n",
    "    \n",
    "    if text:\n",
    "        # Position text at midpoint\n",
    "        mid_x, mid_y = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "        txt = ax.text(mid_x, mid_y, text,\n",
    "                     ha='center', va='bottom',\n",
    "                     fontsize=text_size, fontweight='bold',\n",
    "                     color=color,\n",
    "                     bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    return arrow\n",
    "\n",
    "def draw_cylinder(ax, x, y, width, height, color, label='', text_size=10):\n",
    "    \"\"\"Draw cylinder (for datasets)\"\"\"\n",
    "    # Main body\n",
    "    rect = Rectangle((x, y), width, height * 0.8,\n",
    "                    facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "    # Top ellipse\n",
    "    ellipse_top = Wedge((x + width/2, y + height * 0.8), width/2, 0, 180,\n",
    "                       facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(ellipse_top)\n",
    "    \n",
    "    # Bottom ellipse\n",
    "    ellipse_bottom = Wedge((x + width/2, y), width/2, 180, 360,\n",
    "                          facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(ellipse_bottom)\n",
    "    \n",
    "    if label:\n",
    "        ax.text(x + width/2, y + height/2, label,\n",
    "               ha='center', va='center',\n",
    "               fontsize=text_size, fontweight='bold',\n",
    "               color=COLORS['text'])\n",
    "\n",
    "def draw_neural_network(ax, x, y, width, height, layers=[8, 6, 4], label=''):\n",
    "    \"\"\"Draw simplified neural network diagram\"\"\"\n",
    "    n_layers = len(layers)\n",
    "    layer_spacing = width / (n_layers + 1)\n",
    "    \n",
    "    for i, n_nodes in enumerate(layers):\n",
    "        layer_x = x + (i + 1) * layer_spacing\n",
    "        node_spacing = height / (n_nodes + 1)\n",
    "        \n",
    "        for j in range(n_nodes):\n",
    "            node_y = y + (j + 1) * node_spacing\n",
    "            circle = Circle((layer_x, node_y), 0.15,\n",
    "                          facecolor=COLORS['accent3'],\n",
    "                          edgecolor='black', linewidth=1.5)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Draw connections to next layer\n",
    "            if i < n_layers - 1:\n",
    "                next_n_nodes = layers[i + 1]\n",
    "                next_layer_x = x + (i + 2) * layer_spacing\n",
    "                next_node_spacing = height / (next_n_nodes + 1)\n",
    "                \n",
    "                for k in range(next_n_nodes):\n",
    "                    next_node_y = y + (k + 1) * next_node_spacing\n",
    "                    ax.plot([layer_x, next_layer_x], [node_y, next_node_y],\n",
    "                           'k-', alpha=0.2, linewidth=0.5)\n",
    "    \n",
    "    if label:\n",
    "        ax.text(x + width/2, y - 0.3, label,\n",
    "               ha='center', va='top',\n",
    "               fontsize=10, fontweight='bold',\n",
    "               color=COLORS['text'])\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 1: COMPLETE ARCHITECTURE OVERVIEW\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n1. Generating complete architecture overview...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(0, 14)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(10, 13.5, 'BurstMAE-MIL Architecture',\n",
    "       ha='center', va='top', fontsize=24, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "ax.text(10, 13.0, 'Self-Supervised Learning + Multiple Instance Learning for Fault Diagnosis',\n",
    "       ha='center', va='top', fontsize=14, fontweight='normal',\n",
    "       color=COLORS['text'], style='italic')\n",
    "\n",
    "# ===== STAGE 1: DATA INPUT =====\n",
    "y_start = 11.5\n",
    "draw_box(ax, 0.5, y_start, 2, 0.8, 'Raw AE Signal\\n1 MHz, 1D',\n",
    "        COLORS['data'], text_size=10, edge_color=COLORS['accent1'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 2.5, y_start + 0.4, 3.5, y_start + 0.4, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# ===== STAGE 2: EVENT-BASED SEGMENTATION =====\n",
    "draw_box(ax, 3.5, y_start - 0.5, 3, 1.8, \n",
    "        'Event-Based Segmentation\\n\\n' + \n",
    "        '1. Short-Time Energy\\n' +\n",
    "        '2. MAD Threshold\\n' +\n",
    "        '3. Peak Detection\\n' +\n",
    "        '4. Burst Extraction',\n",
    "        COLORS['preprocessing'], text_size=9, edge_color=COLORS['accent2'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 6.5, y_start + 0.4, 7.5, y_start + 0.4, 'M=32 bursts', COLORS['arrow'], width=3)\n",
    "\n",
    "# ===== STAGE 3: STFT TRANSFORMATION =====\n",
    "draw_box(ax, 7.5, y_start, 2.5, 0.8,\n",
    "        'STFT Transform\\n128×128 patches',\n",
    "        COLORS['preprocessing'], text_size=10, edge_color=COLORS['accent2'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 10, y_start + 0.4, 11, y_start + 0.4, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# ===== STAGE 4: MAE PRE-TRAINING (SELF-SUPERVISED) =====\n",
    "mae_y = 8.5\n",
    "mae_x = 11\n",
    "\n",
    "# MAE box\n",
    "draw_box(ax, mae_x, mae_y, 8, 2.5,\n",
    "        '', COLORS['mae'], text_size=10, edge_color=COLORS['accent3'], edge_width=3)\n",
    "\n",
    "ax.text(mae_x + 4, mae_y + 2.2, 'Masked Autoencoder (Self-Supervised Pre-training)',\n",
    "       ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# MAE components\n",
    "# Encoder\n",
    "draw_box(ax, mae_x + 0.5, mae_y + 0.5, 2.5, 1.5,\n",
    "        'Vision Transformer\\nEncoder\\n\\n6 layers\\n192-dim\\n3 heads',\n",
    "        COLORS['accent3'], text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Masking\n",
    "draw_box(ax, mae_x + 3.5, mae_y + 0.5, 2, 1.5,\n",
    "        'Burst-Aware\\nMasking\\n\\n60% ratio\\nEnergy-based',\n",
    "        '#FFE082', text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Decoder\n",
    "draw_box(ax, mae_x + 6, mae_y + 0.5, 2, 1.5,\n",
    "        'Lightweight\\nDecoder\\n\\n2 layers\\n128-dim',\n",
    "        COLORS['accent3'], text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Reconstruction loss\n",
    "ax.text(mae_x + 4, mae_y + 0.1, 'MSE Loss (Masked Patches)',\n",
    "       ha='center', va='top', fontsize=10, fontweight='bold',\n",
    "       color='red')\n",
    "\n",
    "# Arrow from STFT to MAE\n",
    "draw_arrow(ax, 15, y_start + 0.4, 15, mae_y + 2.5, 'Unlabeled\\nBursts', COLORS['arrow'], width=3)\n",
    "\n",
    "# ===== STAGE 5: SUPERVISED FINE-TUNING (MIL) =====\n",
    "mil_y = 5.5\n",
    "mil_x = 11\n",
    "\n",
    "# MIL box\n",
    "draw_box(ax, mil_x, mil_y, 8, 2,\n",
    "        '', COLORS['mil'], text_size=10, edge_color=COLORS['accent4'], edge_width=3)\n",
    "\n",
    "ax.text(mil_x + 4, mil_y + 1.7, 'Multiple Instance Learning (Supervised Fine-tuning)',\n",
    "       ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# MIL components\n",
    "# Frozen encoder\n",
    "draw_box(ax, mil_x + 0.5, mil_y + 0.3, 2.5, 1.2,\n",
    "        'Frozen MAE\\nEncoder\\n(Transfer)',\n",
    "        '#B39DDB', text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Attention pooling\n",
    "draw_box(ax, mil_x + 3.5, mil_y + 0.3, 2.5, 1.2,\n",
    "        'Attention Pooling\\n\\nw = softmax(attn(H))\\nz = Σ w·H',\n",
    "        COLORS['accent4'], text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Classifier\n",
    "draw_box(ax, mil_x + 6.5, mil_y + 0.3, 1.5, 1.2,\n",
    "        'Classifier\\nFC → 4',\n",
    "        COLORS['accent4'], text_size=9, edge_color='black', edge_width=2)\n",
    "\n",
    "# Arrow from MAE to MIL\n",
    "draw_arrow(ax, 15, mae_y, 15, mil_y + 2, 'Learned\\nFeatures', COLORS['arrow'], width=3)\n",
    "\n",
    "# ===== STAGE 6: OUTPUT =====\n",
    "draw_arrow(ax, mil_x + 8, mil_y + 0.9, mil_x + 9, mil_y + 0.9, '', COLORS['arrow'], width=3)\n",
    "\n",
    "draw_box(ax, mil_x + 9, mil_y + 0.5, 1.5, 0.8,\n",
    "        'Prediction\\nBF/GF/TF/N',\n",
    "        COLORS['output'], text_size=10, edge_color='red', edge_width=3)\n",
    "\n",
    "# ===== DATA FLOW ANNOTATION (LEFT SIDE) =====\n",
    "# Input data\n",
    "draw_box(ax, 0.5, 9.5, 2, 1.2,\n",
    "        'Training Data\\n\\n280 files\\n(70% split)',\n",
    "        COLORS['data'], text_size=9, edge_color=COLORS['accent1'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 1.5, 9.5, 1.5, y_start, '', COLORS['arrow'], width=2, style='fancy')\n",
    "\n",
    "# SSL data annotation\n",
    "ax.text(1, mae_y + 1.2, '~9,000 bursts\\n(unlabeled)', \n",
    "       ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Supervised data annotation\n",
    "ax.text(1, mil_y + 1, '280 bags\\n(labeled)', \n",
    "       ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "# ===== KEY INNOVATIONS (BOTTOM) =====\n",
    "innov_y = 3\n",
    "ax.text(10, innov_y + 1.2, 'Key Innovations',\n",
    "       ha='center', va='top', fontsize=14, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "innovations = [\n",
    "    ('1', 'Event-Based Segmentation', 'Robust MAD threshold'),\n",
    "    ('2', 'Burst-Aware Masking', 'Energy-weighted sampling'),\n",
    "    ('3', 'Self-Supervised Pre-training', 'Unlabeled data leverage'),\n",
    "    ('4', 'Attention MIL', 'Weakly-supervised learning')\n",
    "]\n",
    "\n",
    "for i, (num, title, desc) in enumerate(innovations):\n",
    "    x_pos = 2.5 + i * 4\n",
    "    draw_box(ax, x_pos, innov_y, 3.5, 0.8,\n",
    "            f'{num}. {title}\\n{desc}',\n",
    "            '#FFF9C4', text_size=8, edge_color='black', edge_width=1.5)\n",
    "\n",
    "# ===== PARAMETERS (RIGHT SIDE) =====\n",
    "param_x = 0.5\n",
    "param_y = 5\n",
    "ax.text(param_x + 1, param_y + 2, 'Parameters',\n",
    "       ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "       color=COLORS['text'],\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "params_text = \"\"\"\n",
    "Burst Extraction:\n",
    "- Window: 256 samples\n",
    "- K_MAD: 6.0\n",
    "- Burst length: 4096 samples\n",
    "- Bursts/file: 32\n",
    "\n",
    "STFT:\n",
    "- NFFT: 512\n",
    "- Hop: 128\n",
    "- Output: 128×128\n",
    "\n",
    "MAE:\n",
    "- Encoder: 6 layers, 192-dim\n",
    "- Decoder: 2 layers, 128-dim\n",
    "- Mask ratio: 60%\n",
    "- Epochs: 10, Batch: 128\n",
    "\n",
    "MIL:\n",
    "- Attention: 128-dim hidden\n",
    "- Classes: 4 (BF/GF/TF/N)\n",
    "- Epochs: 30, Batch: 16\n",
    "\"\"\"\n",
    "\n",
    "ax.text(param_x + 0.1, param_y + 1.5, params_text,\n",
    "       ha='left', va='top', fontsize=7, fontfamily='monospace',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "# ===== LEGEND (BOTTOM RIGHT) =====\n",
    "legend_elements = [\n",
    "    mpatches.Patch(facecolor=COLORS['data'], edgecolor='black', label='Data Input/Output'),\n",
    "    mpatches.Patch(facecolor=COLORS['preprocessing'], edgecolor='black', label='Preprocessing'),\n",
    "    mpatches.Patch(facecolor=COLORS['mae'], edgecolor='black', label='Self-Supervised Learning'),\n",
    "    mpatches.Patch(facecolor=COLORS['mil'], edgecolor='black', label='Supervised Learning'),\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='lower right', \n",
    "         fontsize=10, frameon=True, title='Component Types',\n",
    "         title_fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"complete_architecture.png\"), \n",
    "           dpi=1200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"  ✓ Saved: complete_architecture.png\")\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 2: DETAILED MAE ARCHITECTURE\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n2. Generating detailed MAE architecture...\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 18)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(9, 9.5, 'Masked Autoencoder (MAE) Architecture',\n",
    "       ha='center', va='top', fontsize=20, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# Input\n",
    "draw_box(ax, 1, 7, 2, 1.5, 'Input STFT\\nPatch\\n128×128×1', \n",
    "        COLORS['data'], text_size=10, edge_color=COLORS['accent1'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 3, 7.75, 4, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Patch Embedding\n",
    "draw_box(ax, 4, 7, 2, 1.5, 'Patch Embed\\n16×16\\n→ 192-dim\\n(64 patches)',\n",
    "        COLORS['preprocessing'], text_size=9, edge_color=COLORS['accent2'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 6, 7.75, 7, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Masking\n",
    "draw_box(ax, 7, 6.5, 2, 2.5, \n",
    "        'Burst-Aware\\nMasking\\n\\n' +\n",
    "        'Energy = Σ|patch|²\\n' +\n",
    "        'P(mask) ∝ Energy\\n\\n' +\n",
    "        'Keep: 25 patches\\n' +\n",
    "        'Mask: 39 patches',\n",
    "        '#FFE082', text_size=8, edge_color='orange', edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 9, 7.75, 10, 7.75, 'Visible\\npatches', COLORS['arrow'], width=3, text_size=8)\n",
    "\n",
    "# Position Embedding\n",
    "draw_box(ax, 10, 7.5, 1.5, 0.5, '+ Pos Embed',\n",
    "        '#E1BEE7', text_size=8, edge_color='black', edge_width=1.5)\n",
    "\n",
    "draw_arrow(ax, 11.5, 7.75, 12.5, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Transformer Encoder\n",
    "draw_box(ax, 12.5, 6.5, 2.5, 2.5,\n",
    "        'Transformer\\nEncoder\\n\\n' +\n",
    "        '6 × [\\n' +\n",
    "        '  LayerNorm\\n' +\n",
    "        '  Multi-Head Attn\\n' +\n",
    "        '  MLP (4× expand)\\n' +\n",
    "        ']',\n",
    "        COLORS['accent3'], text_size=8, edge_color=COLORS['accent3'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 13.75, 6.5, 13.75, 5.5, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Decoder input prep\n",
    "draw_box(ax, 12.5, 4.5, 2.5, 0.8,\n",
    "        'Add Mask Tokens\\n(39 masked positions)',\n",
    "        '#FFCCBC', text_size=8, edge_color='black', edge_width=1.5)\n",
    "\n",
    "draw_arrow(ax, 13.75, 4.5, 13.75, 3.5, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Transformer Decoder\n",
    "draw_box(ax, 12.5, 2, 2.5, 1.5,\n",
    "        'Transformer\\nDecoder\\n\\n2 layers\\n128-dim',\n",
    "        COLORS['accent3'], text_size=8, edge_color=COLORS['accent3'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 13.75, 2, 13.75, 1, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Prediction Head\n",
    "draw_box(ax, 12.5, 0.2, 2.5, 0.6,\n",
    "        'Linear → 16×16 pixels',\n",
    "        COLORS['accent3'], text_size=8, edge_color='black', edge_width=1.5)\n",
    "\n",
    "draw_arrow(ax, 15, 0.5, 16, 0.5, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Output\n",
    "draw_box(ax, 16, 0, 1.5, 1,\n",
    "        'Reconstructed\\nPatches',\n",
    "        COLORS['output'], text_size=9, edge_color='red', edge_width=2)\n",
    "\n",
    "# Loss annotation\n",
    "ax.text(13.75, 1, 'MSE Loss\\n(masked patches only)',\n",
    "       ha='center', va='center', fontsize=9, fontweight='bold',\n",
    "       color='red',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Dimensions annotation\n",
    "dim_text = \"\"\"\n",
    "Dimensions Flow:\n",
    "\n",
    "Input: (B, 1, 128, 128)\n",
    "   ↓\n",
    "Patches: (B, 64, 256)\n",
    "   ↓\n",
    "Embed: (B, 64, 192)\n",
    "   ↓\n",
    "Masked: (B, 25, 192)\n",
    "   ↓\n",
    "Encoder: (B, 25, 192)\n",
    "   ↓\n",
    "+ Mask Tok: (B, 64, 128)\n",
    "   ↓\n",
    "Decoder: (B, 64, 128)\n",
    "   ↓\n",
    "Head: (B, 64, 256)\n",
    "   ↓\n",
    "Unpatch: (B, 1, 128, 128)\n",
    "\"\"\"\n",
    "\n",
    "ax.text(1, 4, dim_text,\n",
    "       ha='left', va='top', fontsize=7, fontfamily='monospace',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcyan', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"mae_detailed_architecture.png\"), \n",
    "           dpi=1200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"  ✓ Saved: mae_detailed_architecture.png\")\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 3: DETAILED MIL ARCHITECTURE\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n3. Generating detailed MIL architecture...\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 18)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(9, 9.5, 'Multiple Instance Learning (MIL) Architecture',\n",
    "       ha='center', va='top', fontsize=20, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# Input bag\n",
    "draw_box(ax, 1, 7, 2, 1.5, 'Input Bag\\nM=32 bursts\\n(per file)',\n",
    "        COLORS['data'], text_size=10, edge_color=COLORS['accent1'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 3, 7.75, 4, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Individual burst processing\n",
    "draw_box(ax, 4, 7, 2, 1.5, 'STFT\\nTransform\\n32 × 128×128',\n",
    "        COLORS['preprocessing'], text_size=9, edge_color=COLORS['accent2'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 6, 7.75, 7, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Frozen encoder\n",
    "draw_box(ax, 7, 6.5, 3, 2.5,\n",
    "        'Frozen MAE\\nEncoder\\n(Pre-trained)\\n\\n' +\n",
    "        'Vision Transformer\\n' +\n",
    "        '6 layers, 192-dim\\n\\n' +\n",
    "        'Input: (32, 1, 128, 128)\\n' +\n",
    "        'Output: (32, 192)',\n",
    "        '#B39DDB', text_size=9, edge_color=COLORS['accent3'], edge_width=3)\n",
    "\n",
    "draw_arrow(ax, 10, 7.75, 11, 7.75, 'H ∈ ℝ³²ˣ¹⁹²', COLORS['arrow'], width=3, text_size=8)\n",
    "\n",
    "# Attention mechanism\n",
    "draw_box(ax, 11, 6.5, 3, 2.5,\n",
    "        'Attention Pooling\\n\\n' +\n",
    "        'a = MLP(H)\\n' +\n",
    "        'a ∈ ℝ³²\\n\\n' +\n",
    "        'w = softmax(a)\\n' +\n",
    "        'w ∈ ℝ³²\\n\\n' +\n",
    "        'z = Σᵢ wᵢ · Hᵢ\\n' +\n",
    "        'z ∈ ℝ¹⁹²',\n",
    "        COLORS['accent4'], text_size=9, edge_color=COLORS['accent4'], edge_width=3)\n",
    "\n",
    "# Attention weights visualization\n",
    "ax.text(12.5, 5.8, 'Learned Attention\\n(which bursts matter)',\n",
    "       ha='center', va='top', fontsize=8, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "draw_arrow(ax, 14, 7.75, 15, 7.75, 'z ∈ ℝ¹⁹²', COLORS['arrow'], width=3, text_size=8)\n",
    "\n",
    "# Classifier\n",
    "draw_box(ax, 15, 7, 2, 1.5,\n",
    "        'Classifier\\n\\nFC(192 → 4)\\n\\nSoftmax',\n",
    "        COLORS['accent4'], text_size=9, edge_color=COLORS['accent4'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax, 17, 7.75, 17.5, 7.75, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Output\n",
    "draw_box(ax, 1, 3.5, 2, 1.5,\n",
    "        'Prediction\\n\\nP(BF)\\nP(GF)\\nP(TF)\\nP(N)',\n",
    "        COLORS['output'], text_size=9, edge_color='red', edge_width=2)\n",
    "\n",
    "# Loss\n",
    "ax.text(2, 2.5, 'Cross-Entropy Loss',\n",
    "       ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "       color='red',\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7))\n",
    "\n",
    "# Bag visualization\n",
    "bag_y = 4\n",
    "ax.text(9, bag_y + 1.5, 'Bag Representation (32 instances)',\n",
    "       ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# Draw small boxes representing instances\n",
    "for i in range(8):  # Show 8 out of 32\n",
    "    x_pos = 5 + i * 1.2\n",
    "    color = '#90CAF9' if i % 2 == 0 else '#FFE082'\n",
    "    draw_box(ax, x_pos, bag_y, 1, 0.8, f'B{i+1}',\n",
    "            color, text_size=7, edge_color='black', edge_width=1)\n",
    "\n",
    "ax.text(13.5, bag_y + 0.4, '... (24 more)',\n",
    "       ha='center', va='center', fontsize=9, style='italic')\n",
    "\n",
    "# Attention weights visualization\n",
    "att_y = 2.5\n",
    "ax.text(9, att_y + 1.5, 'Attention Weights Example',\n",
    "       ha='center', va='top', fontsize=12, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "# Draw bars representing attention weights\n",
    "weights = [0.15, 0.08, 0.25, 0.05, 0.12, 0.10, 0.18, 0.07]  # Example weights\n",
    "for i, w in enumerate(weights):\n",
    "    x_pos = 5 + i * 1.2\n",
    "    height = w * 3  # Scale for visualization\n",
    "    rect = Rectangle((x_pos + 0.1, att_y), 0.8, height,\n",
    "                     facecolor='green', edgecolor='black', linewidth=1, alpha=0.7)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x_pos + 0.5, att_y - 0.2, f'{w:.2f}',\n",
    "           ha='center', va='top', fontsize=7)\n",
    "\n",
    "ax.plot([5, 14.5], [att_y, att_y], 'k-', linewidth=1)\n",
    "ax.text(14.8, att_y, '0.0', ha='left', va='center', fontsize=8)\n",
    "\n",
    "# Key concepts\n",
    "concept_x = 1\n",
    "concept_y = 0.5\n",
    "concepts = \"\"\"\n",
    "Key MIL Concepts:\n",
    "\n",
    "- Weak Supervision: Only file-level labels\n",
    "- Bag = Collection of instances (bursts)\n",
    "- Attention = Learn which instances matter\n",
    "- Permutation Invariant: Order doesn't matter\n",
    "\"\"\"\n",
    "\n",
    "ax.text(concept_x, concept_y, concepts,\n",
    "       ha='left', va='bottom', fontsize=9,\n",
    "       bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"mil_detailed_architecture.png\"), \n",
    "           dpi=1200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"  ✓ Saved: mil_detailed_architecture.png\")\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 4: TRAINING PIPELINE (TWO-STAGE)\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n4. Generating two-stage training pipeline...\")\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = GridSpec(2, 1, figure=fig, height_ratios=[1, 1], hspace=0.15)\n",
    "\n",
    "# ===== STAGE 1: SELF-SUPERVISED PRE-TRAINING =====\n",
    "ax1 = fig.add_subplot(gs[0])\n",
    "ax1.set_xlim(0, 16)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.axis('off')\n",
    "\n",
    "ax1.text(8, 5.5, 'Stage 1: Self-Supervised Pre-Training (MAE)',\n",
    "        ha='center', va='top', fontsize=18, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor=COLORS['mae'], alpha=0.8))\n",
    "\n",
    "# Unlabeled data\n",
    "draw_box(ax1, 1, 3.5, 2, 1, 'Unlabeled\\nBurst Data\\n~9,000 instances',\n",
    "        COLORS['data'], text_size=9, edge_color=COLORS['accent1'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax1, 3, 4, 4, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# MAE training\n",
    "draw_box(ax1, 4, 3, 4, 2,\n",
    "        'MAE Training\\n\\n' +\n",
    "        '• Mask 60% patches\\n' +\n",
    "        '• Reconstruct masked regions\\n' +\n",
    "        '• MSE loss\\n' +\n",
    "        '• 10 epochs, batch=128',\n",
    "        COLORS['mae'], text_size=9, edge_color=COLORS['accent3'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax1, 8, 4, 9, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# Learned representations\n",
    "draw_box(ax1, 9, 3.5, 2, 1,\n",
    "        'Learned\\nRepresentations\\n192-dim',\n",
    "        '#B39DDB', text_size=9, edge_color=COLORS['accent3'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax1, 11, 4, 12, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# Frozen encoder\n",
    "draw_box(ax1, 12, 3.5, 2, 1,\n",
    "        'Frozen\\nEncoder\\n(Transfer)',\n",
    "        '#B39DDB', text_size=9, edge_color=COLORS['accent3'], edge_width=2)\n",
    "\n",
    "# Annotation\n",
    "ax1.text(8, 2, 'No Labels Required • Learns General Features',\n",
    "        ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "        color='green',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "# Loss curve illustration\n",
    "ax1.plot([1.5, 2.5], [1, 1], 'k-', linewidth=1)\n",
    "ax1.plot([1.5, 2.5], [0.5, 0.5], 'k-', linewidth=1)\n",
    "loss_x = np.linspace(1.5, 2.5, 20)\n",
    "loss_y = 1 - 0.4 * np.exp(-np.linspace(0, 3, 20))\n",
    "ax1.plot(loss_x, loss_y, 'g-', linewidth=2)\n",
    "ax1.text(2, 0.3, 'Loss ↓', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "# ===== STAGE 2: SUPERVISED FINE-TUNING =====\n",
    "ax2 = fig.add_subplot(gs[1])\n",
    "ax2.set_xlim(0, 16)\n",
    "ax2.set_ylim(0, 6)\n",
    "ax2.axis('off')\n",
    "\n",
    "ax2.text(8, 5.5, 'Stage 2: Supervised Fine-Tuning (MIL)',\n",
    "        ha='center', va='top', fontsize=18, fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor=COLORS['mil'], alpha=0.8))\n",
    "\n",
    "# Labeled data\n",
    "draw_box(ax2, 1, 3.5, 2, 1,\n",
    "        'Labeled\\nFile-Level\\n280 bags',\n",
    "        COLORS['data'], text_size=9, edge_color=COLORS['accent1'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax2, 3, 4, 4, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# Feature extraction (frozen)\n",
    "draw_box(ax2, 4, 3.5, 2, 1,\n",
    "        'Feature\\nExtraction\\n(Frozen MAE)',\n",
    "        '#B39DDB', text_size=9, edge_color=COLORS['accent3'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax2, 6, 4, 7, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# MIL training\n",
    "draw_box(ax2, 7, 3, 4, 2,\n",
    "        'MIL Training\\n\\n' +\n",
    "        '• Attention pooling\\n' +\n",
    "        '• Cross-entropy loss\\n' +\n",
    "        '• 30 epochs, batch=16\\n' +\n",
    "        '• Only MIL head trainable',\n",
    "        COLORS['mil'], text_size=9, edge_color=COLORS['accent4'], edge_width=2)\n",
    "\n",
    "draw_arrow(ax2, 11, 4, 12, 4, '', COLORS['arrow'], width=2)\n",
    "\n",
    "# Final model\n",
    "draw_box(ax2, 12, 3.5, 2, 1,\n",
    "        'Final\\nClassifier\\n4 classes',\n",
    "        COLORS['output'], text_size=9, edge_color='red', edge_width=2)\n",
    "\n",
    "# Annotation\n",
    "ax2.text(8, 2, 'File-Level Labels • Learns Task-Specific Features',\n",
    "        ha='center', va='center', fontsize=11, fontweight='bold',\n",
    "        color='blue',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
    "\n",
    "# Accuracy curve illustration\n",
    "ax2.plot([1.5, 2.5], [1, 1], 'k-', linewidth=1)\n",
    "ax2.plot([1.5, 2.5], [0.5, 0.5], 'k-', linewidth=1)\n",
    "acc_x = np.linspace(1.5, 2.5, 20)\n",
    "acc_y = 0.5 + 0.45 * (1 - np.exp(-np.linspace(0, 3, 20)))\n",
    "ax2.plot(acc_x, acc_y, 'b-', linewidth=2)\n",
    "ax2.text(2, 0.3, 'Acc ↑', ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"two_stage_training_pipeline.png\"), \n",
    "           dpi=1200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"  ✓ Saved: two_stage_training_pipeline.png\")\n",
    "\n",
    "# -------------------------\n",
    "# PLOT 5: SIMPLIFIED WORKFLOW DIAGRAM\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n5. Generating simplified workflow diagram...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(0, 20)\n",
    "ax.set_ylim(0, 8)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(10, 7.5, 'BurstMAE-MIL: End-to-End Workflow',\n",
    "       ha='center', va='top', fontsize=22, fontweight='bold',\n",
    "       color=COLORS['text'])\n",
    "\n",
    "y_center = 4\n",
    "\n",
    "# Step 1\n",
    "draw_box(ax, 0.5, y_center - 0.6, 2.5, 1.2,\n",
    "        'Raw Signal\\n1 MHz AE',\n",
    "        COLORS['data'], text_size=10, edge_color=COLORS['accent1'], edge_width=3)\n",
    "ax.text(1.75, y_center - 1.2, '①', ha='center', fontsize=16, fontweight='bold')\n",
    "draw_arrow(ax, 3, y_center, 3.8, y_center, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Step 2\n",
    "draw_box(ax, 3.8, y_center - 0.6, 2.5, 1.2,\n",
    "        'Event-Based\\nSegmentation',\n",
    "        COLORS['preprocessing'], text_size=10, edge_color=COLORS['accent2'], edge_width=3)\n",
    "ax.text(5.05, y_center - 1.2, '②', ha='center', fontsize=16, fontweight='bold')\n",
    "draw_arrow(ax, 6.3, y_center, 7.1, y_center, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Step 3\n",
    "draw_box(ax, 7.1, y_center - 0.6, 2.5, 1.2,\n",
    "        'STFT\\nTransform',\n",
    "        COLORS['preprocessing'], text_size=10, edge_color=COLORS['accent2'], edge_width=3)\n",
    "ax.text(8.35, y_center - 1.2, '③', ha='center', fontsize=16, fontweight='bold')\n",
    "draw_arrow(ax, 9.6, y_center, 10.4, y_center, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Step 4\n",
    "draw_box(ax, 10.4, y_center - 0.6, 2.5, 1.2,\n",
    "        'MAE\\nPre-training',\n",
    "        COLORS['mae'], text_size=10, edge_color=COLORS['accent3'], edge_width=3)\n",
    "ax.text(11.65, y_center - 1.2, '④', ha='center', fontsize=16, fontweight='bold')\n",
    "draw_arrow(ax, 12.9, y_center, 13.7, y_center, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Step 5\n",
    "draw_box(ax, 13.7, y_center - 0.6, 2.5, 1.2,\n",
    "        'MIL\\nFine-tuning',\n",
    "        COLORS['mil'], text_size=10, edge_color=COLORS['accent4'], edge_width=3)\n",
    "ax.text(14.95, y_center - 1.2, '⑤', ha='center', fontsize=16, fontweight='bold')\n",
    "draw_arrow(ax, 16.2, y_center, 17, y_center, '', COLORS['arrow'], width=3)\n",
    "\n",
    "# Step 6\n",
    "draw_box(ax, 17, y_center - 0.6, 2.5, 1.2,\n",
    "        'Fault\\nPrediction',\n",
    "        COLORS['output'], text_size=10, edge_color='red', edge_width=3)\n",
    "ax.text(18.25, y_center - 1.2, '⑥', ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bottom annotations\n",
    "annotations = [\n",
    "    (1.75, 'MAD-based\\nthreshold'),\n",
    "    (5.05, '32 bursts\\nper file'),\n",
    "    (8.35, '128×128\\npatches'),\n",
    "    (11.65, 'Self-supervised\\n9K instances'),\n",
    "    (14.95, 'Attention MIL\\n280 bags'),\n",
    "    (18.25, 'BF/GF/TF/N\\nclasses')\n",
    "]\n",
    "\n",
    "for x, text in annotations:\n",
    "    ax.text(x, y_center + 1, text,\n",
    "           ha='center', va='bottom', fontsize=8,\n",
    "           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', alpha=0.7))\n",
    "\n",
    "# Processing types\n",
    "ax.text(5, 1.5, 'Preprocessing', ha='center', fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.4', facecolor=COLORS['preprocessing'], alpha=0.7))\n",
    "ax.text(11.65, 1.5, 'Self-Supervised Learning', ha='center', fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.4', facecolor=COLORS['mae'], alpha=0.7))\n",
    "ax.text(14.95, 1.5, 'Supervised Learning', ha='center', fontsize=12, fontweight='bold',\n",
    "       bbox=dict(boxstyle='round,pad=0.4', facecolor=COLORS['mil'], alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"simplified_workflow.png\"), \n",
    "           dpi=1200, bbox_inches='tight', facecolor='white')\n",
    "plt.close()\n",
    "\n",
    "print(f\"  ✓ Saved: simplified_workflow.png\")\n",
    "\n",
    "# -------------------------\n",
    "# SUMMARY\n",
    "# -------------------------\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✅ ARCHITECTURE DIAGRAMS COMPLETE!\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nGenerated diagrams:\")\n",
    "print(f\"  1. complete_architecture.png           - Full method overview\")\n",
    "print(f\"  2. mae_detailed_architecture.png       - MAE component details\")\n",
    "print(f\"  3. mil_detailed_architecture.png       - MIL component details\")\n",
    "print(f\"  4. two_stage_training_pipeline.png     - Training procedure\")\n",
    "print(f\"  5. simplified_workflow.png             - High-level workflow\")\n",
    "print(f\"\\nAll saved to: {OUTPUT_DIR}\")\n",
    "print(f\"Resolution: 1200 DPI, publication-ready\")\n",
    "print(f\"Format: PNG with white background\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6b776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
